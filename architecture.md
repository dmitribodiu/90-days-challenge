# ■■■ Day 6 
# Software architecture patterns >>>>>
# Layered architecture
Components within the layered architecture pattern are organized
into horizontal layers, each layer performing a specific role within
the application.
One of the powerful features of the layered architecture pattern is
the `separation of concerns` among components

## Key concepts
1. Layers of isolation
    Means that each layer can only impact layers that are directly besides it.
2. Opened layers
    Some layers (e.g service layers) can be open, meaning you don't need to 
    access them in order to access the layers below.

The layered architecture pattern is a solid general-purpose pattern,
making it a good starting point for most applications

## Drawbacks
1. architecture sinkhole anti-pattern
    When requests flow through multiple layers as simple pass-through 
    with little or no logic performed.
2. Layered architecutre tends to lend itself towards monolithic applications.
    It may cause problems for some applcations.

## Properties
1. Agility - low
    Cumbersome to make changes because of monolithic nature of the pattern.
2. Ease of deployment - low
    Deployment may become an issue for larger apps.
    `Bad for CI pipeline`.
3. Testability - high
    Closed layers are easy to test because it's easy to mock other layers.
4. Performance - low
    Request has to go throught multiple layers which is not efficient.
5. Scalability - low
    Because of tightly couple tendency of this pattern, apps built with it are
    generally difficult to scale.
6. Ease of development - high
    Because layered structure mimics company specialists, it's a default choice 
    for most of companies.

# Event-driven architecture 
The event-driven architecture pattern is a popular distributed
asynchronous architecture pattern used to produce highly `scalable`
applications.

The event-driven architecture is made up of highly decoupled, single-purpose event
processing components that asynchronously receive and process events.

The event-driven architecture pattern consists of two main topologies,
the `mediator and the broker`.

## Mediator topology
The mediator topology is useful for events that have multiple steps
and require some level of orchestration to process the event.

There are four main types of architecture components within the
mediator topology: 
+ event queues
+ event mediator
+ event channels
+ event processors

There are two types of events within this pattern:
+ initial event
    original event received by mediator
+ processing event
    generated by the mediator and received by event processing components
    
For each step in the initial event, the event mediator sends out a specific
processing event to an event channel.

The `event processor` components contain the application business logic
necessary to process the processing event, they are idenpendent, `highly decoupled` 
components that perform a specific task in a system. Each processor should 
perform a single business task and not rely on other processors to complete their task.

## Broker
The broker topology differs from the mediator topology in that
there is no central event mediator, rather, the message flow is distributed
across the event processor components in a chain-like
fashion through a lightweight message broker. 

This topology is useful when you have a relatively
simple event processing flow

There are two main types of architecture components within the
broker topology: 
+ broker component 
+ event processor component.

The event channels contained within the broker component can be
message queues, message topics, or a combination of both.

the broker topology is all about the
`chaining of events to perform a business function`

once an event processor passes the event to another processor,
it is no longer involved with the processing of that specific event.

## Considerations
The event-driven architecture pattern is a relatively complex pattern to implement.
One consideration to take into account when choosing this architecture
pattern is the `lack of atomic transactions` for a single business process.

It is vitally important when
using this pattern to settle on a `standard data format` (XML/JSON)
and establish a contract versioning policy right from the start.

## Properties
1. Agility - high
    Since every process is completely decoupled from other processes, change can be made
    quickly.
2. Deployment - high
    Although broker is a little bit easier to deploy than mediator.
3. Testability - low
    You need special tool to generate events. Asynchronous nature of the pattern
    also adds up to the difficulty.
4. Performance - high
    Due to asynchronous nature is't very performant.
5. Scalability - high
    Each processor can be scaled separately which allows for fine-grained scalability.
6. Ease of development - low
    Why? 
    1. Async 
    2. Need for contract creation (JSON/XML scheme) 
    3. Complicated error handling of failed processors.

# Microkernel architecture
aka `plug-in` architecture pattern is a natural pattern for implementing
product-based applications (apps that are packaged and made available for download)

The microkernel architecture pattern allows you to add additional application
features as plug-ins to the core application, providing extensibility as well
as feature separation and isolation.

The microkernel architecture pattern consists of two types of components:
+ core system 
+ plug-in modules

The core system needs to know which plugins are available and how to get to them.
One way to implement that is through some sort of plug-in registry.
The patterns only specifies that these plugins should be independent from one another.

## Examples
Any IDE with plugins.
Browsers with plugins (Firefox, Chrome)

## Considerations
The microservices architecture pattern provides great support for
evolutionary design and incremental development. You can design the core system
and then as the app evolves add new features without having to make changes to the core
system.

For product-based applications, the microkernel architecture pattern
should always be your first choice. Particularly for products where you will be
releasing additional features and want control over which users get which features.

## Properties
1. Agility - high
    Changes can be isolated and implemented quickly via plug-ins.
2. Deployment - high
    Plugins can be added during runtime
3. Testability - high
    Plug-ins can be tested in isolation
4. Performance - high
    You can include only the plugins you need.
5. Scalability - medium
    Core system is not very scalable, although you can implement scalability on 
    the plug-in level
6. Ease Development - low
    This patterns requires thoughtful design and contract governance, making it 
    rather complex to implement.
    
# Microservices architecture pattern
Each component of this architecture is deployed as a separate unit.

`Service components` is an important notion in microservice architecture.
They contain one or more modules that represent either a single purpose function
or an independent portion of a large business app. 

Designing the right level of service component granularity is
one of the biggest challenges within a microservices architecture.

Another important notion is that microservices is a `distributed architecture`
all components are completely decoupled from one another and are accessed through 
some remote access protocol.

Microservices naturally evolved from layered architecutre and service-oriented 
pattern. This is the next step in patterns evolution.

When you deploy a monolitic application there's a high chance that something
will break. With microservices on the other hand you can deploy more frequently 
and can have more confidence in release quality.

Microservices were designed to be easier than SOA to implement, it's attained
by simplifying connectivity and access to to service components.

## Topologies
In theory you can implement microservices however you want but 
the most common way to implement the patten is by using:
1. API REST based topology
    For websites with specific purposes. That comprise of small services
    that implement some small self-contained portion of business logic.
2. application REST based topology
3. Centralized messaging topology
    Instead of using REST uses a `lightweight centralized message brocker`
    The `benefits` of this topology over the simple REST-based topology are:
    + advanced queuing mechanisms
    + asynchronous messaging
    + monitoring
    + error handling
    + better overall load balancing and scalability.
 
## Avoid dependencies and orchestration
One of the main challenges of the microservices architecture pattern
is determining the correct level of granularity for the service components.

If you find you need to orchestrate your components from user interface - your 
architecture is too fine-graned.
If you need to access multiple components within a single component - your components
are too large.

One way to prevent calling multiple components from within a component is 
to create a `shared database`. This way your services might have repeating 
code for database access, but it's a common practice,
you exchange redundancy of code for decoupling.

## Properties
1. Agility - high
    App built with this pattern tend to be very loosely coupled.
2. Ease of deployment - high
    Easy to deploy due to decoupled nature of pattern
3. Testability - high
    Due to separation of business logic into small components, they are easy to test.
4. Performance - low
    Due to distributed nature of the pattern it's usually not very performant.
5. Scalability - high
    Each component can be individually scaled.
6. Ease of development - high
    High decoupling -> easy development

# Space-based architecture (aka cloud architecture)
Usually architectures are hard to scale.
The space-based architecture pattern is specifically designed to
address and solve scalability and concurrency issues.

The space-based pattern (cloud architecture pattern)
minimizes the factors that limit application scaling

High scalability is achieved by removing the central database constraint and using
replicated in-memory data grids instead.
Because there is no central database, the database bottleneck is
removed, providing near-infinite scalability within the application.

The space-based architecture pattern is a complex and expensive
pattern to implement. It is a good architecture choice for smaller
web-based applications with variable load

However, it is not well suited for traditional
large-scale relational database applications with large amounts
of operational data.

# ■■■ Day 7
# Righting software >>>>>
# Intro
In general design is not time-consuming. If you allocate too much time
for design you risk to add to design things that add nothing but complexity to the design.
Limiting the design time forces you to produce `good-enough` design.

## Eliminating analysis paralysis
1. Design decision `trees`
    Each leaf should be a consistent, distinct and valid solution for a requirement.
    
    When you try to make a new desision about architecture from scratch you
    do something like `bubble sort` which is highly inefficient because it
    doesn't account for previous desisions.
2. Software system design decision tree
    Only after you have designed the system there's a point in designing a project
    to build that system.

    One of the most valuable techniques to decrease the size of a decision tree
    is the application of `constraints`. If you don't have constraints you have
    too many options which is bad (you will spend a lot of time too choose)
    With enough amount of constraints you don't need to design anything, it is what it 
    is.
    
> The clean canvas is the worst design problem. (no constraints)

# System Design <<<
# ■ Desomposition
While designing the system is quick and inexpensive compared with building the system,
it is critical to get the architecture right.

The correct decomposition is critical. A wrong decomposition means wrong architecture.
In modern systems `services` are the most granular unit of the architecture.
However, the technology used to implement the components and their details
are detailed design aspects, not system architecture. 

## Avoid functional decomposition
Functional decomposition decomposes a system into its building
blocks based on the functionality of the system. (Like creating a distinct service
for every operation you need to perform e.g : billing, shipping, checkout)
`Why avoid?`
1. It couples services to the requirements, any change in functionality imposes a change 
    on a service. Functional decomposition precludes reuse
    (you won't be able to use this service in another system)
    and leads to overly compolex systems
    (if you choose to create a single service match-all)

    Functional decomposition, therefore, tends to make services either too big and too few
    or too small and too many. You often see both afflictions in the same system.

2. Clients bloat and coupling
    Functional decomposition often leads to flattening of the system hierarchy.
    Since each service serves specific functionality, someone has to combine these
    discrete functionalities into a required behaviour. That someone is often the client.

    By bloating the client with the orchestration logic,
    you pollute the client code with the business logic of sequencing, ordering,
    error compensation, and duration of the calls to the services. Which prevents
    you from ability to `evolve` client and services `independently`.
    Ultimately, the client is no longer the client—it has become the system.
    
    If there are multiple clients you are destined to duplicate that orchestration 
    logic on all clients making maintenance of all those clients wasteful and expensive.
    As the functionality changes, you now are forced to keep up with that change across
    multiple clients, since all of them will be affected.

3. Multiple points of entry
    Another problem of functional decomposition is that it requires 
    multiple points of entry to the system. The clients need to enter the system 
    in three places: once for the A, then for the B, then for the C service.
    When you will need to change any of these aspects you will need to change it 
    in multiple places across services and clients.

4. Services bloating and coupling
    You can think of letting services call one another instead of multiple points
    of entry for a client. This way you leave only one way of entry for a client 
    which might seem much better.
    The `problem` now is that the functional services are coupled to each
    other and to the `order` in which every service calls each other.

    When you let services know about one another and about order
    in which they should be invoked you couple them so tightly that 
    they become on big fused mess of a service.
    
## Reflecting on functional decomposition
Functional decomposition seems like the perfect way to design a system.
No wonder why so many systems are designed this way.
At all costs, you must `resist the temptations` of functional decomposition.

> Nature of the U niverse
    Functional desomposition is ineffective because it's so simple: just 
    divide the system into requirements and you are done. You can't make a good 
    architecture without breaking a sweat.

Functional decomposition has it's place when trying to figure out requirements 
from the customer. However there should `never` be direct mapping between the
requirements and the design.

## Avoid domain decomposition
Domain decomposition is decomposing a system into building blocks
based on the business domains. The reason domain decomposition does not work is that
it is still functional decomposition in disguise.

1. Building a Domain house
    Imagine completely finishing building a single room, you have one room
    with electricity, water and apartment repair. Then you go to your customer
    and show him "release 1.0". Then to build another room you need to `completely`
    rebuild the first one. 
    
    That's how domain decomposition works in pracitce. You can't just build single 
    domain components by themselves without proper architecture. Because if you 
    do, every time you introduce a new component you will likely need to rewrite 
    all other components which increases complexity to `!n`.

## Faulty motivation
The motivation for functional or domain decomposition is that
the business or the customer wants its feature as soon as possible.
The problem is that `you can never deploy a single feature in isolation`.

## Testability and design
A crucial flaw of both functional and domain decomposition has to do with testing. 
With such designs, the level of coupling and complexity is so high
that the only kind of testing developers can do is unit testing. 
The sad reality is that unit testing is borderline `useless`. While unit testing is
an essential part of testing, it cannot really test a `system`.

even if the complex system is at a perfect state of impeccable quality,
changing a single, unit-tested component could break
some other unit relying on old behavior.

The only way to verify change is `full regression testing` of the system
But functional decomposition makes the entire system untestable in terms of 
regression testing (because with so many units it's hard to create a complete list 
of regression tests), and untestable systems are always rife with defects.

## Physical versus software systems
The main difference between physical and software systems is that when person
builds a physical thing it't obvious to everyone whether the thing is well architected
or not. But with software it's not so obvious. Bad architechture is being maintained
by less experienced engineers, slowing down their growth and in some cases ruining their
careers. 

## Example: Functional Trading System
If you design components so that client has to orchestrate them, every change in
any component will likely affect the client as well. Whatever change you may want to 
make will cause substantional rewrite of the existing system.

## Volatility-based decomposition
`Decompose based on volatility`.

Volatility-based decomposition identifies `areas of potential change` and
encapsulates those into services or system building blocks. You then implement
the required behavior as the interaction between the encapsulated areas of volatility

With volatility-based decomposition you can think of your system as of series of vaults
that contain granades (changes) than may potentially cause problems to your app.

With functional decomposition, your building blocks represent
areas of functionality, not volatility so when change happens it may affect multiple
components in your architecture. Functional decomposition doesn't account for `changes`.

## Decomposition, maintenance and development
As explained previously, functional decomposition drastically increases the
system’s complexity. Functional decomposition also makes maintenance a `nightmare`.
It makes maintaining the code labor intensive, error prone, and very time-consuming. 
Even during development it may easily break your deadline because changes are often made
even while development.

## Universal principle
The merits of volatility-based decomposition are not specific to software systems. 
A functional decomposition of your own body would have components for every
task you are required to do, from driving to programming to presenting,
yet your body does not have any such components. 

> Decomposing based on `volatility` is the essence of system design.

All well-designed systems, software and physical systems alike,
encapsulate their volatility inside the system’s building blocks.

# ■■■ Day 8
## Volatility-Based Decomposition and Testing
Volatility-based decomposition lends well to `regression` testing as well as unit testing.
The reduction in the number of components, the reduction in the size of components,
and the simplification of the interactions between components all drastically
reduce the complexity of the system. 

## The Volatility Challenge
The main challenges in performing a volatility-based decomposition have to do with
+ time
+ perception
+ communication

The outside world (be it customers, management, or marketing) always presents
you with requirements in terms of functionality: “The system should do this and that.”
Consequently, `volatility-based decomposition takes longer`
compared with functional decomposition

The whole purpose of requirements analysis is to `identify the areas of volatility`
and this analysis requires effort and sweat.

1. The 2% problem
    If you sick 1 week a year, that's about 2% of your time.
    Would you go to doctor or try to heal yourself? The moral is if you only have 
    to spend 2% of your time at any complex task you will never be good at it.
    Architects should find a way to get more time to architect systems.

2. The Dunning-Kruger effect
    If you are unskilled in something, you never assume it is
    more complex than it is, you assume it is less!

## Identifying volatility
1. Volatile vs variable
    Variability can be handled easily using conditional logic.
    Volatility has a potential to change your whole system logic.

2. Axes of volatility
    Finding areas of volatility is a process of discovery that takes place
    during requirements analysis and interviews with the project stakeholders.

    In any business, there are only two ways (axes) your system could face change: 
        + at the same customer over time. 
        + at the same time over customers
    If something doesn't map to this axes you should not encapsulate it at all
    as it goes back to functional decomposition.

3. Design factoring
    Often, the act of looking for areas of volatility using the axes of
    volatility is an `iterative process`.

4. Independence of the axes
    Two axes should be independent.
    
## Example: Volatility-based decomposition of a house
You may chose these areas of volatility for the first axes (same customer over time): 
    appliances, occupants, furniture, appearance.
And these for the second axes (same time over customers):
    structure, neighbours, location
    
The assignment of a volatility to one of the axes is not an absolute
exclusion but more one of disproportional `probability`.

The axes of volatility are a great starting point,
but it is not the only tool to bring to bear on the problem.

## Solutions masquerading as requirements
E.g when customer says that he wants a kitchen, what he really does is providing
you with a solution to feeding people in the house.

With volatility-based decomposition, during requirements analysis,
you should identify the volatility in feeding the occupants and provide for it
(what's volatile here is the way you provide the food to occupants)

The volatility of feeding is encapsulated within the `Feeding`,
component and as the feeding options change, your design does not.

Since most requirements specifications are chock-full of solutions masquerading
as requirements, functional decomposition absolutely maximizes your pain.
You will forever be chasing the ever-evolving solutions, never recognizing
the true underlying requirements.

## Volatility examples
+ user volatility
+ Client app volatility
    web/mobile
+ security volatility
    different methods of authentication
+ notification volatility
    email/phone/browser
+ storage volatility
    local db / cloud 
    (local db is actually a disguised solution to a requirement of storing data)
+ connection and synchronization volatility
+ locale and regulations volatility
+ input volatility

Is vital to call out the areas of volatility and map them
in your decomposition `as early as possible`

1. System decomposition
    Once you have settled on the areas of `volatility`,
    you need to `encapsulate` them in components of the architecture
    The transition from the list of volatile areas to components of
    the architecture is hardly ever one to one.
    
    With design, always start with the `simple and easy decisions`.
    Those decisions `add constrains` to the system, making subsequent decisions easier.

## Volatility And The Business
While you must encapsulate the volatile areas, not everything that could change
should be encapsulated. During system decomposition, you must identify both
the areas of volatility to encapsulate and those not to encapsulate.
The first indicator is that the possible change is `very rare`.
The second indicator is that any attempt to encapsulate the change can only be done poorly

A change to the `nature` of the business permits you to kill
the old system and start from scratch. You can do it because it happens very rare and
in case it doesn't happen you will save a lot of money that you would otherwise
spend on expensive encapsulating details.

1. Speculative design
    Is a variation on trying to encapsulate the `nature` of the business
    When taken to the extreme, you run the risk of trying to encapsulate anything and
    everywhere, you get some kind of paranoia : "everything can change".
    Your design will have numerous building blocks, a `clear sign of a bad design`.

## Volatility and longevity
The longer the company or the application has been doing something the same way,
the higher the likelihood the company will keep doing it the same way.

## The importance of practicing
The best way of going about mastering volatility-based decomposition is to `practice`.
    + practice on a system that you are familiar with
    + examine your own `past` projects
    + look at your `current` project, it may not be too late to save it
    + look at physical things made by engineers, how do they incapsulate volatility
        (e.g : bicycle, car, computers, TV, smartphones)

Then `do it again and do it some more`.
You better fail in your own projects than on live projects.

# ■ Structure
Fortunately, all systems share common areas of volatility.

## Use cases and requirements
Before diving into architecture, consider requirements.
Requirements should capture the `required behavior` rather than the required functionality
You should specify how the system is required to operate as opposed to what it should do

1. Required behaviours
    A use case is an expression of required behavior—that is,
    how the system is required to go about accomplishing some work
    and adding value to the business. As such, a use case is a particular
    `sequence of activities` in the system.

    You can capture use cases either textually or graphically.
    But because human brain is better prepared for visual information
    the best way of capturing a use case is `graphically`, with a diagram.
    Humans perform image processing astonishingly quickly

    Graphical use cases, however, can be very labor-intensive to produce,
    especially in large numbers. For the most part you shold represent graphically
    usecases that contain "if" in them. Because nested if's are hard to imagine
    without a graphical representation.
    
    1. Activity diagrams
        You should prefer activity diagrams for graphical representation of use cases.
        primarily because they can capture `time-critical` aspects of behavior 
        which allows you to easily represent asynchronous behaviour.

## Layered approach
Software systems are typically designed in layers.
Each layer encapsulates its own volatilities from the layers above
and the volatilities in the layers below. Services inside the layers encapsulate
volatility from each other. 

1. Using services
    The preferred way of crossing layers is by calling `services`. 
    When you use services you gain the following benefits:
        1. Scalability
        2. Security
            All service-oriented platforms treat security as a first-class aspect.
        3. Throughput and availability
            Services can accept calls over queues, allowing you to handle a very
            large volume of messages by simply `queuing` up the excess load.
        4. Responsiveness
            Clients and services can use some reliable messaging protocol
            to guarantee delivery, handle network connectivity issues,
            and even order the calls.
        5. Consistency
        6. Synchronization

## Typical layers
You should consider these four main layers:
    1. Client
    2. Business logic
    3. Resource access
    4. Resource
    (5. Utility `bar`, with helper mehtods like logging, security, message bus and so on)

1. Client layer 
    All clients use the same entry point to the system.
    The client layer encapsulates the potential volatility in Clients
    and allows to develop each client separately. (each client doesn't affect any other)

2. Business logic layer
    The business logic layer encapsulates the volatility in the system’s business logic
    which is the volatility of use cases. A particular use case can change in only
    two ways: 
        + The sequence itself changes 
        + The activities within the use case change
        
    `Both` the sequence and the activities are volatile, and these volatilities 
    should be encapsulated in `specific components`: 
        + **Managers** 
            Should encapsulate vol. in the sequence
        + **Engines**.
            Encapsulate volatility in activity.
    Engines may be shared between Managers because you could perform an activity
    in one use case on behalf of one Manager and then perform the same activity
    for another Manager in a separate use case.
    You should design Engines with `reuse` in mind.
    
    However, if two Managers use two different Engines to perform the same activity,
    you likely have functional decomposition on your hands

3. Resource access layer
    encapsulates the volatility in accessing a resource. Over time, you may want
    to change the way you access the database, so that change or the volatility
    involved should be encapsulated. You also have to not only encapsulate the way
    you access a resource, you also have to encapsulate the vol. in resource itself
    which can be a local, cloud, sql, nosql database.
    
    you must avoid operations such as Open(), Close(), Seek(), Read(), and Write() 
    because these operations are specific to certain kinds of resource access.
    A well-designed ResourceAccess component exposes in its contract
    the `atomic business verbs` around a resource.

    1. Use atomic business verbs
        Atomic business verbs are low level `activities` that can not be expressed 
        by any other activity in the system. E.g Bank may have transfer high level
        activity that comprises of two low level activities: debit and credit. These
        are two atomic business verbs. Although they are atomic for business they may need
        multiple steps to perform in your system.
        
        Resource access layer should `only` expose `these` atomic business verbs to the 
        higher layer.

    2. Resource access reuse
        Resource access services can and should be `shared between managers and engines`
        if you have trouble sharing the same resource access endpoint for the same
        purpose between managers and engines you have probably failed to identify
        an atomic business verb.

4. Resource layer
    contains the actual physical `Resources` on which the system relies
    (such as database or message queue)
    Often, the Resource is a whole system in its own right,
    but to your system it appears as just a Resource.

5. Utilities bar
    contains Utility services, which are some form of common infrastructure
    that nearly all systems require to operate.
    
    Utilities require `different rules` compared with the other components.

## Classification guidelines
1. What's in a name
    Service `names` as well as diagrams are important in communicating your design
    to others. You should follow the following conventions for naming services:
        + names of services should be two-part compound words in PascalCase.
        + suffix of the service is always the service type (manager/engine/access)
        + prefix 
            + for managers - a noun associated with encapsulated volatility
            + for engines - a noun describing encapsulated activity
            + for resource access - noun associated with a resource
        + gerunds should only be used in engines (because they describe activity)
        + atomic business verbs should not be used in prefix for a service name.
            they should only appear in operation names in resource access layer.

    e.g : 
        + BillingManager 
            bad, managers should not contain gerunds, they don't describe actions
            they should describe orchestration of actions. Better one: PaymentManger
            (encapsualtes volatility in payment process)
        + CalculatingEngine
            good
        + AccountEngine
            bad, not clear what activity it incapsulates.
            better one: CalculatingEngine

2. The four questions
    When you are not sure about where to start the design, you can start by answering
    four questions about the system:
        1. who
        2. what
        3. how
        4. where

3. Managers to engines ratio
    If your design contains a large number of Engines, you probably have
    done a functional decomposition. Because there is naturally not that many 
    atomic operations in any business.

    Typically there should be more managers than engines and they typically
    maintain the `golden ratio` (managers = engines * 1.61). But if you have
    too many manabers (6+) it may be a sign of a bad design. The large number of Managers
    strongly indicates you have done a functional or domain decomposition.

4. Key observations
    1. Volatility decreses top-down
        In a well-designed system, volatility should decrease top-down across the layers.
        Clients are very volatile. Managers do change, but not as much as their Clients.
        Managers change when the use cases change.
        Engines are less volatile than Managers.For an Engine to change, your business
        must change the way it is performing some activity.
        Resources are the least volatile components.
        A design in which the volatility decreases down the layers is extremely `valuable`
    
    2. Reuse `increases` top-dows
        Managers are reusable because you can use the same Manager and use cases
        from multiple Clients. Engines are even more reusable than Managers because
        the same Engine could be called by multiple Managers.
    
    3. Proper managers
        If the Manager merely `orchestrates` the Engines and the ResourceAccess,
        encapsulating the sequence volatility, you have a great Manager service.
        
## Subsytems and services
A cohesive interaction between the Manager, Engines, and ResourceAccess
may constitute a single logical service to external consumers. 
You can view such a set of interacting services as a logical `subsystem`. 
(which will still consist of 4 layers, divided `vertically`,
where each vertical slice implements a corresponding set of use cases.)

`Avoid over-partitioning` your system into subsystems.
Most systems should have only a handful of subsystems.
You should limit the number of Managers per subsystem to `three`. 

1. Incremental construction
    In case of building a large-scale application it makes sense to develop
    and deliver the system in stages, one `vertical` slice at a time,
    as opposed to providing a single release at the end of the project.

    With both small and large systems, the right approach to construction
    is another `universal principle`:

    **Design iteratively, build incrementally**

    Meaning design can change because you don't yet know the final result.
    But when you build you need to know what will be the end result of development.
    There are `two reasons` why you can build only incrementally, and not iteratively:
        1. Building iteratively is horrendously `wasteful`.
        2. Intermediate iterations don't have any business value.
    
    Building iteratively vs building incrementally
        + iteratively - build 4 floor house but with poor quality, bad walls, bad floors 
            and so on. Then for the next iteration destroy the existing house and build
            new one with a new architecture that will support higher quality.
        + incrementally - build 1 floor properly and then go to the second one 
            and do it properly each time, maintaining the same architecture.
        
    - Extensibility
        The vertical slices of the system also enable you to accommodate extensibility. 
        If you have designed correctly for extensibility, you can mostly
        leave existing things alone and extend the system as a whole by adding more 
        `vertical` slices.
        
2. About microservices
    There are no microservices—only services. 
    1. History and concerns
        In common usage, microservices correspond to domains or subsystems—that is,
        to the vertical slices.
        There are three problems with this idea as practiced today:
            1. Implied constraint on the number of services 
            2. The second problem is the widespread use of functional
                decomposition in microservice design by the industry at large.
                Maintainable, reusable, extensible services are possible
                just not in this way.
            3. The third problem relates to communication protocols.
                Most systems use REST. However REST was designed for publicly
                facing services, as the gateway to systems.  As a general principle,
                in any well-designed system you should never use the same communication
                mechanism both internally and externally.
            
                The protocol used for external services is typically low bandwidth,
                slow, expensive, and error prone. HTTP may be perfect for external
                services, but this protocol should be avoided between internal
                services where the communication and the services must be impeccable.
                
                Internal services such as Engines and ResourceAccess should rely on fast,
                reliable, high-performance communication channels (like message queue)

## Open and closed architecutres
1. Open architecture
    In an open architecture, any component can call any other component
    regardless of the layer in which the components reside. It gives you greater 
    flexibility at a cost of `high coupling`.
    
    Calling sideways also creates additional coupling. When using open architecture,
    there is hardly any benefit of having architectural layers in the first place.
    In software engineering, trading encapsulation for flexibility is a `bad` trade.

2. Closed architecture
    In a closed architecture, you strive to maximize the benefits of the
    layers by disallowing calling up between layers and sideways within layers.

3. Semi-closed
    Allows to call multiple levels down. 
    This architecture is justified in two cases: 1. You need maximum performance at 
    all costs. 2. Layers hardly ever change.
    
    You should avoid this architecture as in most cases it can't be justified.

4. Relaxing the rules
    For real-life business systems, the best choice is always a `closed` architecture. 
    
    1. Calling utilities
        In a closed architecture, Utilities pose a challenge. If you call them a 
        service, not all layers will have an access to it. But specifically for 
        utilities you should relax the closed architecture rule and allow any service
        to call any utility.
        
        You may see attempts by some developers to abuse the utilities bar by
        calling a Utility any component they wish to short-circuit across
        all layers. Not all components can reside in the utilities bar.
        
        To qualify as a Utility, the component must pass a simple litmus `test`:
        Can it be used in any other complex system?

    2. Calling Resource access by business logic
        Managers and Engines can both call ResourceAccess services without
        violating the closed architecture
    3. Queued manager to manager
        Manager can `queue` a call to another manager,
        which won't count as a sideways call
    4. Opening the architecture
        Even with the best set of guidelines, time and again you
        will find yourself trying to open the architecture.
        
        Instead of violating the architecture you should find a better way to 
        do the same thing `preserving the architecture`. There's always such a way.
        E.g you could harness message queuing or utilities bar. Solid architecture 
        will payoff a lot later.

5. Design dont's
    **DON'T DO THESE THINGS**
    1. Clients can only call managers.
        Otherwise you pollute clients with business logic which will render further
        development hell.
    2. Managers don't queue calls to more than one manager in the same use case.
        Use event listening utility instead.
    3. Only managers can queue or listen to events
        Other services should not care about events. It's not their business.
    4. No service should be able to call it's sibling. (sideway call)

6. Strive for symmetry
    The symmetry in software systems manifests in repeated call patterns across use cases.
    Symmetry is so fundamental for good design that you should generally see the same call
    patterns across Managers.

# ■■■ Day 9
# ■ Composition
How do you know the composition of all the components at run-time
adequately satisfies all the requirements? You can and must be able to produce a
viable design and validate it in a repeatable manner.

## Requirements and changes
Requirements change.

1. Resenting change
    Developers resent changes because when the requirements change,
    their design also must change. In any system, a change to the design is very painful.

2. Design prime directive
    The solution for this dissonance of resenting the changes is so simple
    that it has eluded almost everyone for their entire career:
    **Never design against the requirements**
    Any attempt at designing against the requirements will always guarantee pain.

3. Futility of requirements
    the correct way of capturing the requirements is in the form of `use cases`:
    the required `set of behaviors`(not functions) of the system.
    
    You can't collect a complete and correct list of requirements upfront. requirements
    will change during development.
    
## Composable design
The goal of any system design is to be able to satisfy `all use cases`.
The word all in the previous sentence really means `all`:
    + present
    + future
    + known
    + unknown 
Nothing less will do. If you fail to pass that bar, then at some
point in the future, when the requirements change, your design will have to change which
is a `sign of a bad design` (architecture should remain constant)

1. Core use cases
    The core use cases represent the essence of the business of the system.
    Most systems have as few as two or three core use cases,
    and the number seldom exceeds six.
    1. Finding the core use cases
        A  core use case will almost always be some kind of an `abstraction` of other
        use cases, and it may even require a new term or name to differentiate it from
        the rest. The whole point of requirements analysis is
        to `recognize the core use cases` in customer requirements.
    
2. The Architect's mission
    Your mission as an architect is to identify the `smallest` set of components that
    you can put together to satisfy all the core use cases.
    A composable design does not aim to satisfy any use case in particular.
    1. Architecture validation
        Once you can produce an interaction between your services for each
        `core` use case, you have produced a `valid` design. 

    2. Smallest set
        Remember: Your mission as an architect is to identify not just a set
        of components that you can put together to satisfy all the core use case,
        but the `smallest` set of components.

        The smallest set of services required in a typical software system
        contains number of services that is in order of magnitude `10` (12, 20, 15, 25)
        
        Even in a large system you are commonly looking at two to five Managers,
        two to three Engines, three to eight ResourceAccess and Resources,
        and a half-dozen Utilities.
        
    3. Duration of design effort
        You may spend weeks or months trying to identify the core use
        cases and the areas of volatility. However, that is not design—that is
        requirements gathering and requirements analysis, which may be very
        time-consuming indeed. Design is not time-consuming if you know what you are doing
        You can design in a matter of days, and with practice even faster (hours).

## There's no feature
> Features are always and everywhere `aspects of integration, not implementation`.

Integrating all of automobile parts yields the feature - transporting you from point A to 
point B. You can process text on your laptop but is there any box in
the architecture of the laptop called Word Processing?

The laptop provides the feature of word processing by integrating the keyboard,
the screen, the hard drive, the bus, the CPU, and the memory.

You never should implement feature, you have to implement everything that combined
will provide the feature and much more. This will make your design solid.

## Handling change
Your software system must respond to changes in the requirements and respond to them fast.

The trick to addressing change is not to fight it, postpone it, or put it altogether.
The trick is `containerizing its effects`  (volatility based decomosition to the resque)

# ■ System design example
## Business alignment
architecture does not exist for its own sake.
The architecture (and the system) must serve the business.
You must ensure that the `architecture is aligned with the vision`
that the business has for its future and with the business objectives

1. The vision
    The vision must drive everything, from architecture to commitments. 
2. Business objectives
    After agreeing on the vision (and only then), you can itemize the
    vision to specific objectives. You should reject all objectives that do
    not serve the vision. 
    E.g objective for the TradeMe system:
        + Quick turnaround for new requirements 
        + high degree of customization between countries
        + support full business visibility 
        + streamline security
    
3. `Mission` statement
    The mission is not to build features—`the mission is to build components`
    that will allow for creating `any feature` now and in the future.
    
    **Vision => Objectives => Mission statement => Architecture**

# ■■■ Day 10
# The Design of Web Apis >>>>>
# Designing predictable API
An API can be predictable because it shares similarities that other
users have encountered before. 

## Being consistent
An inconsistent design introduces variations or contradictions that make
an interface harder to understand and use.

Attaining predictability of a design can be done with
a little bit of discipline by ensuring
`consistency of data and goals` inside and across all APIs, using and meeting prescribed
standards, and by shamelessly copying others.

But if consistency can lead to an awesome API design,
it must `never` be used at the expense of `usability`.

1. Designing consistent data.
    designing consistent APIs starts with choosing consistent `names`
    - inconsistent
        + accountNumber
        + source
        
        + balanceDate
        + dateOfCreation
        + executionDay
    - consistent
        + accountNumber
        + sourceAccountNumber
        
        + balanceDate
        + creationDate
        + executionDate
    When choosing names for various representations of the `same concept`,
    take care to use similar ones.
    Using a `generic suffix` (like balance"Date") or prefix in a name to provide
    additional information about the nature of what is named is a `good` practice.
    
    But even correctly named, a property can still be subject to inconsistency
    if using `inconsistent type` format.
    (e.g different date formats for variables with Date suffix above)
    Choose a `naming convention` and follow it strictly.
    
    Each level of a URL should always have the same meaning
    Create `consistent URLs`:
        - inconsistent
            + accounts/:id
            + transfers/delayed/:id     (delayed part is inconsistent)
        - consistent
            + accounts/:id
            + delayed-transfers/:id

    Create consistent `return values`:
        - inconsistent
            get accounts/
                { "items": [{"accountNumber": "0001234567"}] }
            get transfers/
                [{"id": "00135135"}]            <= different `nesting` and identificator
        
        - consistent
            get accounts/ - [{"id": "0001234567"}]
            get transfers/ - [{"id": "0005232235"}]
            
    Basically, `every bit` of an API’s data `must be consistent`.

2. Designing consistent goals
    What’s the problem with the `readAccount` and `getUserInformation` goals? These two
    goal names are inconsistent. Data must be consistent, and so must a goal’s inputs.
    The same goes for the feedback returned in case of success or error return values.
    When designing APIs, you must also take care to create `consistent goal flows`.
    So every single aspect of the interface contract, every behavior of an API,
    must be consistent.
    
3. The four levels of consistentcy
    1. Consisntecy within API
    2. Consistency with the domain of an API
        For marine navigation domain you would not use kilometers but 
        nautical miles instead. For language processing you would use `token` instead
        of word.
    3. Consistency across organization APIs
    4. Consistency with the rest of the world

4. Copying others: Following common practices and meeting standards
    Like any real-world device, an API can take advantage of standards
    (in a broad sense) to be easier to understand
    (almost all real world devices share simillar play button design)
    
    REST APIs can be consistent by simply applying the HTTP protocol’s rules to the
    letter (by using `standartized response codes`)
    
    HTTP protocol provide a consistent framework for REST APIs,
    making them totally predictable (standartized codes and verbs)
    
    Even if not everything is standardized in the API design world 
    (how to create url's properly),
    there are common practices that are very close to being standards.

5. Being consistent is hard and must be done wisely
    You must formally define your design with rules in a document called the
    “API Design Guidelines”. Even if you are the only API designer in the team
    it's very helpful because `human tend to forget things`
    
    Once you have your API design cheat sheet and your API directory
    you can concentrate on solving real problems and not waste your time
    reinventing the wheel you created a few months ago.
    
    Consistency is great but don't sacrifice usability for consistency, 
    you can't always be consistent.

## Being adaptable
Managing different representations of the same concept and providing a partial,
selected, or adapted representations of some content is not reserved for books;
we can do that with APIs too.

1. Providing and accepting different formats
    For some array you could provice json view as well as CSV.
    But if a goal can return a list in various formats, how can
    consumers tell which format is needed? You could take advantage of 
    `content negotiation` using `Accept: FORMAT` header.

    Content negotiation is an HTTP mechanism that allows the exchange
    of different representations of a single resource.
    When an HTTP server responds to a request, it must indicate the media type
    of the returned document.
    This is done in the `Content-type` response header.
    When client sends data within request body, it can also use Content-type
    header to indicate the type of data in the body of the request.
    
2. Internationalizing and localizing
    Content negotiation not only applies to data formats, but also to languages.
    Consumers and server can use `Accept-Language` and `Content-Language` headers
    to indicate which language they are speaking and serving in request/response.
    
    `internationalization` means being able to understand
    that an Accept-Language: fr-FR header means that the consumer wants a localized
    response using French language and conventions(e.g measure of distance).
    
    On the server side, it means that if the requested localization is supported,
    the content will be returned localized along with a Content-Language: fr-FR header.
    If it’s not supported, the server returns a 406 Not Acceptable status code.
    `localization` means being actually able to handle some locale.
    
    If you don’t think you need `i18n` and `l10n`, you can start without
    internationalization features and update your API later if needed.

3. Filtering, paginating, and sorting
    To get specific page in an array of objects you can utilize `Range` http header.
    (e.g : Range: items=10-19)
    You can use any string value instead of "items" but remember about consistency,
    better choose a name for a single item and stick to it.
    
`Filtering` based on some numeric value
I.e you want to list cars with mileage between 15,000 and 30,000 miles.
You can do it this way: 
    1. add range header specifying 15000 - 30000
    2. add a header specifying what property should be inspected

## Being discoverable
Like a table of contents in books, APIs can be designed in order to be discoverable.
This is done by providing additional data in various ways,
specifically discoverability can be improved by taking advantage of the protocol used.
REST APIs have the discoverable feature in their genes because they use HTTP protocol.

1. Providing metadata
    How do consumers know that there are multiple pages available?
    If you only return items, it's like a book without page numbers and table of content.
    This response could be improved by adding some `metadata` about pagination.
    ```json
    {
        "pagination" : {    // <-- pagination
            "page": 1,
            "totalPages": 9
        },
        "items": {          // <-- items themselves
            
        }
    }
    ```
    Even if client didn't request specific page in a request you could supply 
    this metadata anyway, to make your API `discoverable`.
    
    Metadata is not limited to pagination, it can be used to tell the client
        + what operations are available on the object
        + what types of objects there are
        + etc.

2. Creating hypermedia APIs
    Imagine a site doesn't have hyperlinks, would it be pleasurable for you
    to got to site docs and discover all hyperlinks by yourself?
    The World Wide Web without its hypermedia links would be quite terrible to use.
    
    Fortunately, this isn’t how it works. Once on a website,you can discover its content
    simply by clicking links and going from one page to another. You can leverage
    hyperlinks in your api as well: 
    ```json
    {
        "pagination" : {
            "next" : "/accounts/1234567/transactions?page=2",
            "last" : "/accounts/1234567/transactions?page=7"
        }, 
        "items" : {
            
        }
    }
    ```
    There is no standard way to provide this hypermedia metadata,
    but there are common practices.
    
    Hypermedia metadata usually uses names such as 
        + href
        + links
        + _links.

    Although there is no standard, several hypermedia formats have been defined.
    One of the most popular ones are `HAL` and `Siren`.
    A basic HAL document has a `links` property containing the available links
    Each `link` is an object identified by its relationship with the current resource.
    The link object contains at least an `href` property with the full URL or relative URL
    ```json
    {
        "_links" : {
            "self": {
                "href": "/accounts/1234567"
            },
            "transactions": {
                "href":"/accounts/1234567/transactions"
            }
        },
        "id": "1234567",
        "type": "CURRENT",
        "balance": 10345.4,
        "balanceDate": "2018-07-01"
    }
    ```
    Hypermedia APIs do not only provide available URLs they can also provide
    available HTTP `methods`.
    "Siren" allows you to specify http methods for actions, links for releveant resources
    and properties for the item all in different json objects in a single root json.
    ```json
    // example of a siren document
    {
        "properties" : {
            "id": "000001",
            "date": "2135-07-01",
            "source": "1234567",
            "destination": "7654321",
            "amount": "1045.2"
        },
        "links": [
        { 
            "rel": ["self"],
            "href": "/transfers/000001" 
        }],
        "actions": [
        { 
            "name": "cancel",
            "href": "/transfers/000001",
            "method": "DELETE" 
        }],
    }
    ```
    
    Providing hypermedia metadata is the most common way of taking advantage of the
    web roots of REST APIs to create predictable APIs, but the HTTP protocol provides
    features that can be used to make REST APIs even `more predictable`.

3. Taking advantage of HTTP protocol
    `OPTIONS` http method can be used to request available http methods on a resource:
    ```c
    OPTIONS /transfers/000001       // http request
    
    200 OK                          // http response
    Allow: GET, DELETE              // Allow is an HTTP header
    Content-type: application/json  // Specify content type
    Link: "/accounts/1234567/transactions?type=text/csv" // available formats
    ```
    
    Note that such use of the HTTP protocol by REST APIs is `not widespread`.
    You can use such features, but they will have to be carefully `documented`.
    Don't confuse users with complex or totally unused features.
    
# ■■■ Day 11
# Designing concise and well-organized api
Organizing and sizing an API’s data,
feedback, and goals is important in order to provide an API that can be easily understood
and will not overwhelm users.

## Organizing api
Just like an everyday object, an API can be either unusable or perfectly
intuitive, depending on the `organization` of its data, feedback, and goals.

1. Organizing data
    You can `sort` properties of data by their name.
    You can `group` properties by putting them into another object
    ```json
    {
        "age": 3,
        "type": 2,
        "safeToSpend": 600,
        "overdraftProtection": true,
        "limit": 100
    }
    {
        "overdraftProtection": {
            "active": true,
            "limit": 100
        },
        "age": 3,
        "type": 2,
        "safeToSpend": 600,
    }
    ```

2. Organizing feedback
    A well-organized API provides well-organized feedback.
    You should organize your errors when they occur.
    I.e you could assign to every error it's type (business type), source and message.

3. Organizing goals
    OpenAPI Specification can be used to organize an API’s goals.
    You can add `tags`
    ```yaml
    get: 
      tags: 
         - Account
    ```
    The idea is to group together goals that are related from a functional point of view.
    you must focus on the `functionality` of the goals and not their representations
    
    `Sorting by tags`: people will likely want to accesss account functionality 
    before doing any transfers in some banking app, so you can sort goals by tags
    in OpenAPI like this:
    ```yaml
    opanapi: "3.0.0"
    tags:
        - name: Account
          description: "dfdf"
        - name: Transfer
          description: "dfdf"
    ```
    Now all Account goals will go before Transfer.
    
    Now that the groups are sorted, we should also `sort the operations` within the groups
        1. Preserve the order of http verbs: get, post, patch, delete
        2. put same resources together
        ```openapi
        get beneficiaries
        pust beneficiaries
        get beneficiaries/:id
        ```
        3. group resource paths
        ```
        /account/accounts/
        /account/accounts/{id}
        
        /transfer/beneficiaries/
        /transfer/beneficiaries/{id}
        /transfer/transfers/
        ```

## Sizing api
Objects providing too many functions, too many controls, or too much
information are usually not really usable. Sometimes you’ll find that
what you’d considered as a single API can be worth splitting into different ones

1. Choosing data granularity
    Try to restrict the number of properties and the nesting depth of 
    properties you return to the client.

2. Choosing goal granularity
    You need to consider how particular goal should be structured.
    I.e you should not allow a client to manipulate it's transactions 
    through his account endpoint, instead create another endpoint for 
    transactions manipulating.
    `Avoid` creating does-it-all goals
    
3. Choosing API granularity
    Sometimes you may want to split api into multiple services.

# ■■■ Day 12
# Functional domain modelling >>>>>
# Understanding the Domain <<<
# Introducing DDD
Let's look at how to minimize the “garbage-in” by
using a design approach focused on clear communication and shared domain
knowledge: Domain-Driven Design.

DDD is particularly useful for business and enterprise software, where developers
have to collaborate with other nontechnical teams

## The Importance of a Shared Model
how can we ensure that we, as developers, do understand the problem?
A mismatch between the developer’s understanding of the problem and the domain expert’s
understanding of the problem can be fatal to the success of the project.

The goal of domain-driven design is to `share the same mental model of a domain`
among domain experts, architects and developers. 

Aligning the software model with the business domain has a number of benefits:
    1. Faster time to market.
    2. More business value
    3. Less waste
    4. Easier maintenance
        When the model expressed by the code closely matches the domain expert’s own model
        making changes to the code is easier and less error prone.

So we need to create a shared model. How can we do this? We can follow the following 
guideline:
    + Focus on business events and workflows rather than data structures.
    + Partition the problem domain into smaller sub-domains.
    + Create a model of each sub-domain in the solution
    + Develop a common language
    
## Understanding the Domain Through Business Events
Domain events are the starting point for almost all of the business processes
we want to model. It's somethign that forces `data transformation`.
Domain events are always written in the past tense – something happened –
because it’s a fact that can’t be changed.

1. Using Event Storming to Discover the Domain
    Is a collaborative process for discovering business events and their workflows
    
2. Discovering the Domain: An Order-Taking System
    After event storming we might have list of posted events like this:
        + ‘Order form received’
        + ‘Order placed’
        + ‘Order shipped’
        + ‘Order change requested’, ... etc
    It can `help` us with:
        + Finding gaps in the requirements
            When the events are displayed on a wall in a timeline, missing requirements
            often become very clear.
        + Connect teams better
            The events can be grouped in a timeline, and it often becomes clear that
            one team’s output is another team’s input.
        + Awareness of reporting requirements
            Any business needs to understand what happened in
            the past – reporting is always part of the domain!
            
3. Expanding the Events to the Edges
    Extending the events out as far as you can in either direction is another great
    way of `catching missing requirements`. Constantly ask what then? or how did you get 
    there? questios to expand the edges of an event.
    
4. Documenting Commands
    Once we have a number of these events on the wall, we might ask: “`What`
    made these domain events happen?” We call these requests Commands in DDD terminology
    
    `A command triggers an event`, which initiates some business workflow.
    The output of the workflow is some more events. And then, of course, those events can
    trigger further commands.

    By the way, not all events need be associated with a command. Some events
    might be triggered by a scheduler or monitoring system

## Partitioning the Domain into Subdomains
We have now got a list of events and commands, and we have a good understanding
of what the various business processes are.
we can define a `domain` as **an area of coherent knowledge**.

Within a domain, there might be areas which are distinctive as well. We call
these `sub-domains`. Domains can overlap, it is tempting to want clear, crisp boundaries,
but the real world is fuzzier than that

## Creating a Solution Using Bounded Contexts
The solution can’t possibly represent all the information in the original domain,
nor would we want it to. We should only capture the information which is
relevant to solving a particular problem – everything else is irrelevant

To build the solution we will create a model of the problem domain, extracting only
the aspects of the domain that are relevant and then recreating them in our
solution space.

In the real world, domains have fuzzy boundaries, but in
the world of software we want to reduce coupling between separate subsystems
so that they can evolve independently. This means, sadly, that our domain
model will never be as rich as the real world, but we can tolerate this in
`exchange for less complexity and easier maintenance`.

However you partition the domain, it’s important that each bounded context
has a clear responsibility, because when we come to implement the model, a
bounded context will correspond exactly to some kind of software component.

1. Getting the Contexts Right
    one of the most important challenges of a domain-driven
    design is to get these context boundaries right
    here are some guidelines that can help:
        + listent to domain experts
        + pay attention to real world boundaries
            (but don't think this is the best way to partition the system)
        + design for `autonomy`
            so that every subdomain can `evolve independently` 
        + consider business workflow as well
            sometimes it may be better to uglify the design in order to improve 
            some business workflow.
            
2. Creating Context Maps
    Once we have defined these contexts we need a way to communicate the
    interactions between them – `the big picture` – without getting bogged down
    in the details of a design. The goal is not to capture every detail, but
    to provide a view of the system as a whole.
    You can visualize it as a `graph`

3. Focusing on the Most Important Bounded Contexts
    Generally, some domains are more important than others. These are the `core` domains

    Other domains may be required but are not core. These are called _supportive_
    domains, and if they are not unique to the business, are called _generic_
    domains (generic domains can be easily outsourced)

## Creating a Ubiquitous Language
if the domain expert calls something an “Order”
then we should have something called an Order in the code that corresponds
to it and that behaves the same way

And conversely, we should `not` have things in our `design` that do not represent
something in the domain expert’s model.

The set of concepts and vocabulary that is shared between everyone on the
team is called the `Ubiquitous Language`

And, as its name implies, this language should used everywhere in the project,
not just in the requirements but in the design and, most importantly, the source code.

# Understanding the Domain
## Interview with a Domain Expert
The best way to learn about a domain is to pretend you’re
an anthropologist and avoid having any pre-conceived notions

1. Understanding the Non-Functional Requirements
2. Understanding the Rest of the Workflow
3. Thinking About Inputs and Outputs

## Fighting the Impulse to Do Database-Driven Design
In domain-driven design we let the domain drive the design, not a database schema.
the users do not care about how data is persisted.

If you design from the database point of view all
the time, you often end up `distorting` the design to fit a database model.

## Fighting the Impulse to Do Class-Driven Design
We should keep our minds `open` during requirements
gathering and not impose our own technical ideas on the domain

## Documenting the Domain
how should we record these requirements?
We could use some visual language like UML or some language like yaml.

## Representing Complexity in our Domain Model
1. Representing Constraints
2. Representing the Life-cycle of an Order
3. Fleshing Out the Steps in the Workflow

listening to the domain expert carefully reveals a lot of
complexity, even in a relatively simple system

# A Functional Architecture
how should we translate our understanding of the
domain into a software architecture? 

In a fast-paced development cycle, we will often need to start implementing some
of the domain before we have understood the rest of it

A software architecture consists of `four levels`:
    1. system context
        top level representing the entire system
        system context comprises of `containers`
    2. container 
        deployable units such as a website, a web service, a database
        each container comprises of a number of `components`
    3. component
        major structural building block in the code
        each component comprises a number of `classes` (not OOP classes)
    4. class
        structure that contains low-level methods or functions
    
One of the goals of a good architecture is to define the various boundaries
between containers, components, and modules, such that when new
requirements arise, as they will, the “cost of change” is minimized.

## Bounded Contexts as Autonomous Software Components
bounded contexts should stay decoupled and autonomous.

## Communicating Between Bounded Contexts
How do bounded contexts communicate with each other?
You could choose `message queue` communication to reduce coupling.

1. Transferring Data Between Bounded Contexts
    In general, an event used for communication between contexts will not just
    be a simple signal, but will also contain all the data that the downstream
    components need to process the event. (if data is too large, you could pass a ref.)
    To transfer data you could use DTO's (Data transfer objects)

2. Trust Boundaries and Validation
    At the input gate, we will `always validate` the input to make sure that it conforms
    to the constraints of the domain model. If the validation fails, then the rest of
    the workflow is bypassed and an error is generated

    The job of the output gate is different. Its job is to ensure that
    `private information doesn’t leak out of the bounded context`
    (especially for security reasons)
    In order to do this, the output gate will often deliberately “lose” information
    in the process of converting domain objects to DTOs.

## Contracts Between Bounded Contexts
We want to reduce coupling between bounded contexts as much as possible,
but a shared communication format always induces some coupling.
The two contexts will need to agree on a common format for them in order
for communication to be successful.

Who gets to decide the contract? There are 3 patterns:
    1. Consumer driven
        Contract is dictated by a consumer (when consumer plays important business role)
    2. Provider driven
        Contract is dictated by a provider (better decoupling)
    3. Anti-Corruption Layers
        ACL prevents the internal, pure domain model from being `corrupted`
        by knowledge of the outside world. This is a common pattern when
        using a third-party component.

## Workflows Within a Bounded Context
You can imagine a workflow in a context as a single function that has an input and 
an output to and from outside world.

1. Workflow Inputs and Outputs
    The input to a workflow is always the data associated with a command, and
    the output is always a set of events to communicate to other contexts.

    In the diagram above, it’s important to note that a workflow function does
    not “publish” domain events – it simply returns them

2. Avoid Domain Events Within a Bounded Context
    In functional design we `append` event listeners to a context, 
    we don't listen to events inside the same context (as you would do in OOP)
    because it creates hidden dependencies.

## Code Structure Within a Bounded Context
With traditional approach you design by layers. One `big drowback` is that layered
architecture violates one very important rule: 
    “code that changes together, belongs together”.
Because some `change to a workflow` would touch `all` the layers.

Better ways is to switch to `vertical slices`, where each workflow contains all
the code it needs to get its job done, and when the requirements change for
a workflow, only the code in that particular vertical slice needs to change.

1. The Onion Architecture
    Let’s instead put the `domain code at the center`, and then have the other
    aspects be assembled around it using the rule that each layer can only depend
    on `inner` layers, not on layers further out.

2. Keep I/O at the Edges
    A major aim of functional programming is to work with functions that are
    predictable and easy to reason about `without` having to look `inside` them.

    But then, how do we read or write data? The answer is to push any I/O to the edges
    of the onion – to only access a database, say, at the `start or end` of a workflow
    the core domain model is only concerned with business logic, while persistence
    and other I/O is an infrastructural concern, which is an outer layer of the 
    architecture. This helps you design system around domain instead of around database.

# ■■■ Day 13
# Modelling the Domain <<<
By the end of this part, you’ll know how to write concise code that does double duty:
1. `documentation` of the domain
2. compilable framework that the rest of the implementation can build upon
    
# Understanding Types
## Understanding Functions
function is a kind of black box with an input and an output.

1. Type Signatures
    inputType -> outputType descriptions is called `type signature`

2. Functions with Generic Types   
    
## Types and Functions
A type in functional programming is not the same as a class in object-oriented
programming. It is much simpler.

From a conceptual point of view, the things in the type can be any kind of
`thing`, real or virtual. functions are things too, so we can use sets of functions
as a type as well.

## Composition of Types
Is the `foundation` of functional design.
In the functional programming world, we use composition to build new
functions from smaller functions and new types from smaller types.

1. New types from smaller types 
    You can create a new type by `AND`int or `OR`ing them. (as in TS)
    The types that are built using AND are called _Product Types_
    The types that are built using OR are called _Sum Types_

2. Simple types
    We will often define a choice type with only one choice
    
3. Algebraic Type Systems
    an algebraic type system is simply one where every
    compound type is composed from smaller types by ANDing or ORing them together

## Building a Domain Model by Composing Types
```fs
type CardType =
    Visa | Mastercard // create an OR type 
    
type CreditCardInfo = { // 'AND' type (record)
    CardType : CardType // or type inside of AND type
    CardNumber : CardNumber
}
```

Now to document the actions that can be taken, we define
`types that represent functions` (separately, unlike in oop)

```fs
type PayInvoice = UnpaidInvoice -> Payment -> PaidInvoice
// Which means: given an UnpaidInvoice and then a Payment, we can create a PaidInvoice.

type ConvertPaymentCurrency = Payment -> Currency -> Payment
// convert payment from one currency to another
```

## Modeling Optional Values, Errors, and Collections
1. Optional values
    We can model this with a choice type called Option, defined like this:
    ```fs
    type Option<'a> =
        | Some of 'a
        | None
    ```
    Option type can be used to wrap any other type. To indicate optional data in
    the domain model then, we wrap the type in Option<..>
    
2. Modelling errors
    Even is programming language does support throwing exceptions, we will often want
    to `explicitly` document in the type signature the fact that a failure can happen.

    This calls out for a choice type with two cases
    ```fs
    type Result<'Success,'Failure> =
        | Ok of 'Success    // may return result
        | Error of 'Failure // may throw error
    ```
    To indicate that a function can fail, we `wrap` the output with a Result type.
    
3. Modeling No Value at All
    Most programming languages have a concept of `void`
    In functional programming function must always return something.
    In f# this "something" is 'unit', When you see the unit type in a signature,
    that’s a strong indication that there are side-effects. 
    
4. Modeling Lists and Collections
    For domain modelling it's recommended to use some kind of 
    `fixed size immutable collection`

## Organizing Types in Files and Projects
A standard approach is to put all the domain types in one file

# Domain Modelling with Types
Can we use the source code directly like documentation, and
avoid the need for UML diagrams and the like? The answer is `yes`
We’ll see that types can replace most documentation

## Modeling Simple Values
You can `wrap` simple values in a type definition like this:
```fs
type UnitQuantity = UnitQuantity of int
// wrapping a type helps differentiate between different int's that may have 
// different constraints
```

1. Working with Single Case Unions
    Passing different wrapper type of the same inner type shoult not be possible. 

2. Constrained Values
    Almost always, the simple types are `constrained` in some way
    
3. Performance Issues with Simple Types
    Wrapping primitive types into simple types is a `great way` to ensure type
    safety and prevent many errors at compile time. But it may cost you performance.
    
    To prevent it you can use :
        1. Type alias (instead of simple types (in F#))
        2. Use structs instead of types (in F#)

    It’s generally best to model your domain in the most
    `straightforward` way first, and only then work on tuning and optimization.

## Modeling Complex Data
1. Modeling with Record Types
    Instead of AND types you can use record (objects in JS)
    
2. Modeling Unknown Types   
    If you don't know all domain type constraints, this is not a problem –
    you can represent types of unknown structure with best guesses,

3. Modeling with Choice Types
    Choise can be represented in TS with OR types

## Modeling Workflows with Functions
For example, if we have a workflow step that validates an order form, we might
document it as:
```fs
type ValidateOrder = UnvalidatedOrder -> ValidatedOrder
```

1. Working with Complex Inputs and Outputs
    + If you need to return multiple values you can use a record.
    + If you find yourself wanting to return different values from the same function
        reconsider your design.
    + If you need to pass multiple inputs you can also use a record 
        (but sometimes, a couple of parameters would feet better)

2. Documenting Effects in the Function Signature
    + If function may throw and error you should find a way to document it 
        in it's signature. 
    + If a function is async, it should be obvious as well
    + If a function is both async and can throw an error (as most functions)
        result type definition can get ugly, so you can create a type 
        `alias` for this case
    
## A Question of Identity: Value Objects
In DDD terminology, objects with a persistent identity are called `Entities` and
objects without a persistent identity are called `Value Objects`.

Value Objects have no identity, e.g name "Fransua" is the same as another name "Fransua"
because two strings are `interchangeable`.
So you can tell that these are Value Objects in the domain

For value object we don't need any special _comparison logic_

## A Question of Identity: Entities
In a business context, Entities are often a document of some kind: Orders,
Quotes, Invoices etc. 
Manufacturer may have identity codes on a smartphone.
Even is customer changes battery on a phone it should still be the same phone for 
manufacturer. (it case of warranty)

Entities need to have a `stable identity` despite any changes,
and therefore, when modeling them, we need to give them a unique identifier or key

In FP Entities and Value Object should be immutable and that's a great thing.

## Aggregates
Aggregate is a type that comprises of other types. Such as types are immutable
`we can't change inner type` without creating a new copy of an agregate.

1. Aggregates Enforce Consistency and Invariants
    An aggregate plays an important role when data is updated.
    when one part of the aggregate is updated, other parts might also need to be
    updated to ensure consistency

    Aggregate contains logic about it's inner fields, e.g when you want to 
    delete some value it may prevent you from doing it if it aligns with business rules
    of the aggregate.
    
2. Aggregate References
    But aggregates should only enforce consistency over it's simple types,
    complex types that are in turn aggregates as well can enforce consistency
    themselves. 
    A much better design in this case is just to store a `reference`
    to another aggregate.
    
    e.g you want to access info about customer from order.
    in this case you clearly should not store customer as a field, reference would 
    make much more sence

3. Aggregates are the basic unit of percistence
    If you want to load or save objects from a database, you should
    load or save whole `aggregates`.
    
# Integrity and Consistency in the Domain
We want to make sure that any data in domain is valid and consistent.
The goal is to create a `bounded context` that always contains data
we can trust, as distinct from the untrusted outside world.

If we can be sure that all data is always valid, the implementation can stay `clean`
and we can avoid having to do defensive coding.

1. Integrity
    Means that a piece of data follows the correct business rules.
2. Consistency
    means that different parts of the domain model agree about facts.
    e.g: 
        total must be the sum of parts.
        When an order is placed, a corresponding invoice must be created.
        (if invoice is not created -> the data is inconsistent)
        
> the more information we can capture in the type system, the less documentation is needed

## Integrity of Simple Values
When creating simple values (custom types for simple values)
the whole point of doint it is to add `constraints`.
When we have constraints we don't have to check the value multiple times.

The constraints should be enforced when the type is created.
(You should put type creation logic near the type)

## Enforcing Invariants with the Type System
An `invariant` is a condition that stays true no matter what else happens.
Let's suppose that there must always be at least one order line in an order
To make sure that a list is not empty, we just need to define a `NonEmptyList` type.
The definition itself requires that there must always be at least one element,
so a NonEmptyList is guaranteed never to be empty.
With this change the constraint “there is always at least one order line in an
order” is now enforced `automatically`.

## Capturing Business Rules in the Type System
Can we document business rules using just the type system?

Let's suppose we want to introduct different emails: verivied and unverified 
and only send reset-password mails to verified emails.
We could just add `verified` field to the email entity,
but this approach has a number of serious `problems`:
    1. It’s not clear when or why the IsVerified flag should be set or unset.
        It's easy to forget to update the property when it's needed by the business rules
    2. There's possibility of security breach.
        
So what should you do?
When domain experts talk about “verified” and “unverified” emails,
you should model them as `separate things`.
```fs
// bad, we still have to check manually, and furthermode, the type may be corrupted
type CustomerEmail =
    | Unverified of EmailAddress
    | Verified of EmailAddress
    
// better way
type CustomerEmail =
    | Unverified of EmailAddress
    | Verified of VerifiedEmailAddress
```
This is an example of the important design guideline: 
    `make illegal states unrepresentable`
That means that if I have a new email address, I have to construct a CustomerEmail
using the Unverified case. and `the only way` I can get a VerifiedEmailAddress
is from the email `verification service` itself.

We are trying to capture business rules in the type system.
If we can do this properly, invalid situations cannot ever exist in the code and
we never need to write unit tests for them – we have `compile-time unit tests` instead.

Another important benefit of this approach is that it actually documents the domain better
`Rather than having a simplistic EmailAddress that tries to serve two roles`
we have two distinct types.

With this definition, we don’t have to worry about someone accidentally
passing in a normal EmailAddress and breaking the business rule because they
haven’t read the documentation. (genius)

## Consistency
“Consistency” as described here is a business term, not a technical one, and
what consistency means is always context dependent.
Consistency does place a large burden on the design though, and can be
costly, so we want to avoid the need for it if we can.

1. Consistency Within a Single Aggregate
    It is clear that the only component that “knows” how to preserve consistency
    is the top-level aggregate. This is a good reason for
    doing all updates at the aggregate level rather that at the line level

2. Consistency Between Different Contexts
    e.g: When user adds word with a new tag, new tag must be created, if a word is
    created but tag is not, the data is inconsistent
    
    You could first ask "tagService" to create needed tags and only then add the 
    work with the needed tag. This is a perfectly valid solution 
    
    But sometimes it may be better to allow inconsistent data is the possibility of
    error is too big and enforcing consisncy would decrease speed of transactions 
    significantly. 
    

You shouldn’t feel obligated to reuse aggregates if it
doesn’t make sense to do so. If you need to make a new aggregate like this
just for one use-case, go ahead.
    
# ■■■ Day 14
# Understanding Functional programming <<<
## What is FP?
we express what to do rather than how to do it

1. Containers, Functors, Lists, and Streams
    Functor is a data structure that provides an interface for itarating all it's members.
    
2. Declarative vs Imperative
    Declarative programs abstract the flow control process
    
    Imperative code frequently utilizes statements. A statement is a piece
    of code which performs some action.    

    Declarative code relies more on expressions.
    An expression is a piece of code which evaluates to some value.

Functional programming `favors`:
    + pure functions
    + immutability
    + functions composition over imperative flow control
    + generics
    + declarative code
    + `expressions` over statements

3. Currying
    A curried function is a function that takes multiple parameters `one at a time`
    It takes a parameter, and returns a function that takes the next parameter,
    and so on until all parameters have been supplied

4. Composition
    Of course you can compose functions. Function composition is the
    process of passing the return value of one function
    as an argument to another function: `f(g(x))`

## Curry and Function Composition
1. What is?
    A curried function is a function that takes multiple arguments one at a time.
    ```js
    const add = a => b => a + b;
    ```
2. What is partial application?
    A partial application is a function which has been applied to some,
    but not yet all of its arguments.
    
    Partial applications can take as many or as few arguments a time as desired.
    Curried functions on the other hand always return a unary function:
    a function which takes one argument.

3. What is point-free style?
    Point-free style is a style of programming where function definitions
    do not make reference to the function’s arguments.
    ```js
    const add = a => b => a + b;
    
    const inc = add(1);
    const inc10 = add(10);
    
    inc(3); // 4
    inc10(55); // 65
    ```
    This ways you have one argument predefined.

4. Why do we curry?
    Curried functions are particularly useful in the context of `function composition`.
    
5. How to debug point-free style functions? - `Trace`
    ```js
    const pipe = (...fns) => x => fns.reduce((y, f) => f(y), x);
    const trace = label => value => { console.log(label+':'+value); return value; }
    const i = n => n + 1;
    const d = n => n * 2;
    
    const h = pipe(
        g,
        trace('after g'),
        f,
        trace('after f'),
    );
    ```

6. Curry + Composition
    The reason that curried functions are so convenient for function composition is
    that they transform functions which expect multiple parameters into functions
    which can take a single argument, allowing them to fit in a function 
    composition pipeline.

## Abstraction & Composition
> Software solutions should be decomposable into their component parts,
> and recomposable into new solutions, without changing the internal component
> implementation details.

1. Abstraction is simplification
    The process of abstraction has two main components:
        1. `Generalization` is the process of finding similarities
        2. `Specialization` is the process of using the abstraction,
            supplying only what is different

2. Abstraction in Software
    Abstraction in software takes many forms: {algorithms, data str, frameworks}
    Functions make great abstractions

3. Abstraction through composition
    The most useful functions for abstraction in software are `pure` functions

4. How to Do More with Less Code
    Abstraction is the key. Starting with useful abstractions as our building blocks,
    we can construct fairly complex behavior with very little new code. E.g :
    ```js
    const add => a => b => a+b; // add is an abstraction
    const inc => add(1); // inc is a specialization 
    ```

5. Characteristics of good abstractions
    + Composable
    + Reusable
    + Independent
    + Concise
    + Simple

## Functors & Categories
A functor data type is something you can `map` over. It’s a container which has
a map operation which can be used to apply a function to the values inside it.
Using a functor is easy — just call .map().

1. Why Functors?
    + iteration abstraction
    + easy for composition
    + can be used for streams of data as well as for arrays

## Monads
A monad is a way of composing functions that require context in addition
to the return value. Monads can compose:
```js
a => M(b), b => M(c)  //becomes
a => M(c)
```

Monad is a `type` of functor. 
    Functor -> implements map 
    Monad -> implements `flatMap` as well as map
    
What is flatMap?
    Example:
        Streams are functors so you can use map on them.
        But is stream contains promises you will need to first unfold promise 
            into value.
        And only then can you map the value.
    FlatMap not only maps, but also `unfolds` the values contained in the
    collection before mapping. Promises are kinda like monads.

+ `Map` means "apply a function to a and return b"
+ `Context` is the computational detail of the monad. 
    The functor or monad supplies some computation to be performed 
    during the mapping process. The point of functors and monads is to abstract that
    context away so we don’t have to worry about it while we’re composing operations.
+ `Type lift` is something like wrapping value in a Promise
+ `Flatten` is an opposite of type lift, "unwrap"
+ `flatMap` is the operation that defines a monad.
    It combines map and flatten into a `single` operation
    used to compose type lifting functions (a => M(b)).

If your map function lifts a type, e.g:
```js
const echo = n => v => Array.from({ length: n }).fill(x);
```
You need to flatMap it in order to map it later.
Otherwise you will get collection of lifted types.
```js
const coll = [1,2,3];
coll.map(echo(3))
    .map(double); // won't work, double gets an array as parameter

coll.flatMap(echo(3))   // 1,1,1,2,2,2,3,3,3
    .map(double);       // works as intended

// vague implementation
class Collection {
    flatMap(f) {
        [...this.collection].reduce((b, v) => [b,...f(v)], []); // base, value
    }
}
```

1. What monads are made of?
    A monad is based on a simple symmetry
    Combine map(typeLifting) with flatten(typeDescending), and you get `flatMap`
    ```js
    const Monad = value => ({
        flatten: f => f(value),
        flatMap (f) {
            return this.flatten(a => Monad.of(f(a)));
        }
    });
    Monad.of = x => Monad(x);
    ```

## OOP history
Lambda calculus represents a `top-down`, function application approach to computation
Turing machine represents a bottom-up, imperative (step-by-step) approach to computation.

Both imperative programming and functional programming have their roots in the mathematics
of computation theory, predating digital computers.

The big idea is `messaging`.

1. Essence of OOP
    + encapsulating state
        The only way to affect another object’s state is by sending a message.
        
    + decoupling through message passing
        the message sender is only loosely coupled to the message receiver,
        through the messaging API.
        
    + runtime adaptability (dynamic binding)
        you can swap modules at runtime.

2. What is not OOP
    + classes
    + polymorphism
    + recognizing class as a "type"

## Functional mixins
Functional mixins are composable `factory` functions which connect together in a pipeline
Simply pass any arbitrary object into a
mixin, and an enhanced version of that object will be returned.
Advantages:
    + encapsulation
    + inheriting private state
    + multiple source inheritance
        + last in wins
    + no base-class requirement

Mixins are a `form of object composition`, where component features get mixed
into a composite object so that properties of each mixin become properties
of the composite object.

## Why composition is harder with classes



































































