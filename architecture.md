# ■■■ Day 6 
# Software architecture patterns >>>>>
# Layered architecture
Components within the layered architecture pattern are organized
into horizontal layers, each layer performing a specific role within
the application.
One of the powerful features of the layered architecture pattern is
the `separation of concerns` among components

## Key concepts
1. Layers of isolation
    Means that each layer can only impact layers that are directly besides it.
2. Opened layers
    Some layers (e.g service layers) can be open, meaning you don't need to 
    access them in order to access the layers below.

The layered architecture pattern is a solid general-purpose pattern,
making it a good starting point for most applications

## Drawbacks
1. architecture sinkhole anti-pattern
    When requests flow through multiple layers as simple pass-through 
    with little or no logic performed.
2. Layered architecutre tends to lend itself towards monolithic applications.
    It may cause problems for some applcations.

## Properties
1. Agility - low
    Cumbersome to make changes because of monolithic nature of the pattern.
2. Ease of deployment - low
    Deployment may become an issue for larger apps.
    `Bad for CI pipeline`.
3. Testability - high
    Closed layers are easy to test because it's easy to mock other layers.
4. Performance - low
    Request has to go throught multiple layers which is not efficient.
5. Scalability - low
    Because of tightly couple tendency of this pattern, apps built with it are
    generally difficult to scale.
6. Ease of development - high
    Because layered structure mimics company specialists, it's a default choice 
    for most of companies.

# Event-driven architecture 
The event-driven architecture pattern is a popular distributed
asynchronous architecture pattern used to produce highly `scalable`
applications.

The event-driven architecture is made up of highly decoupled, single-purpose event
processing components that asynchronously receive and process events.

The event-driven architecture pattern consists of two main topologies,
the `mediator and the broker`.

## Mediator topology
The mediator topology is useful for events that have multiple steps
and require some level of orchestration to process the event.

There are four main types of architecture components within the
mediator topology: 
+ event queues
+ event mediator
+ event channels
+ event processors

There are two types of events within this pattern:
+ initial event
    original event received by mediator
+ processing event
    generated by the mediator and received by event processing components
    
For each step in the initial event, the event mediator sends out a specific
processing event to an event channel.

The `event processor` components contain the application business logic
necessary to process the processing event, they are idenpendent, `highly decoupled` 
components that perform a specific task in a system. Each processor should 
perform a single business task and not rely on other processors to complete their task.

## Broker
The broker topology differs from the mediator topology in that
there is no central event mediator, rather, the message flow is distributed
across the event processor components in a chain-like
fashion through a lightweight message broker. 

This topology is useful when you have a relatively
simple event processing flow

There are two main types of architecture components within the
broker topology: 
+ broker component 
+ event processor component.

The event channels contained within the broker component can be
message queues, message topics, or a combination of both.

the broker topology is all about the
`chaining of events to perform a business function`

once an event processor passes the event to another processor,
it is no longer involved with the processing of that specific event.

## Considerations
The event-driven architecture pattern is a relatively complex pattern to implement.
One consideration to take into account when choosing this architecture
pattern is the `lack of atomic transactions` for a single business process.

It is vitally important when
using this pattern to settle on a `standard data format` (XML/JSON)
and establish a contract versioning policy right from the start.

## Properties
1. Agility - high
    Since every process is completely decoupled from other processes, change can be made
    quickly.
2. Deployment - high
    Although broker is a little bit easier to deploy than mediator.
3. Testability - low
    You need special tool to generate events. Asynchronous nature of the pattern
    also adds up to the difficulty.
4. Performance - high
    Due to asynchronous nature is't very performant.
5. Scalability - high
    Each processor can be scaled separately which allows for fine-grained scalability.
6. Ease of development - low
    Why? 
    1. Async 
    2. Need for contract creation (JSON/XML scheme) 
    3. Complicated error handling of failed processors.

# Microkernel architecture
aka `plug-in` architecture pattern is a natural pattern for implementing
product-based applications (apps that are packaged and made available for download)

The microkernel architecture pattern allows you to add additional application
features as plug-ins to the core application, providing extensibility as well
as feature separation and isolation.

The microkernel architecture pattern consists of two types of components:
+ core system 
+ plug-in modules

The core system needs to know which plugins are available and how to get to them.
One way to implement that is through some sort of plug-in registry.
The patterns only specifies that these plugins should be independent from one another.

## Examples
Any IDE with plugins.
Browsers with plugins (Firefox, Chrome)

## Considerations
The microservices architecture pattern provides great support for
evolutionary design and incremental development. You can design the core system
and then as the app evolves add new features without having to make changes to the core
system.

For product-based applications, the microkernel architecture pattern
should always be your first choice. Particularly for products where you will be
releasing additional features and want control over which users get which features.

## Properties
1. Agility - high
    Changes can be isolated and implemented quickly via plug-ins.
2. Deployment - high
    Plugins can be added during runtime
3. Testability - high
    Plug-ins can be tested in isolation
4. Performance - high
    You can include only the plugins you need.
5. Scalability - medium
    Core system is not very scalable, although you can implement scalability on 
    the plug-in level
6. Ease Development - low
    This patterns requires thoughtful design and contract governance, making it 
    rather complex to implement.
    
# Microservices architecture pattern
Each component of this architecture is deployed as a separate unit.

`Service components` is an important notion in microservice architecture.
They contain one or more modules that represent either a single purpose function
or an independent portion of a large business app. 

Designing the right level of service component granularity is
one of the biggest challenges within a microservices architecture.

Another important notion is that microservices is a `distributed architecture`
all components are completely decoupled from one another and are accessed through 
some remote access protocol.

Microservices naturally evolved from layered architecutre and service-oriented 
pattern. This is the next step in patterns evolution.

When you deploy a monolitic application there's a high chance that something
will break. With microservices on the other hand you can deploy more frequently 
and can have more confidence in release quality.

Microservices were designed to be easier than SOA to implement, it's attained
by simplifying connectivity and access to to service components.

## Topologies
In theory you can implement microservices however you want but 
the most common way to implement the patten is by using:
1. API REST based topology
    For websites with specific purposes. That comprise of small services
    that implement some small self-contained portion of business logic.
2. application REST based topology
3. Centralized messaging topology
    Instead of using REST uses a `lightweight centralized message brocker`
    The `benefits` of this topology over the simple REST-based topology are:
    + advanced queuing mechanisms
    + asynchronous messaging
    + monitoring
    + error handling
    + better overall load balancing and scalability.
 
## Avoid dependencies and orchestration
One of the main challenges of the microservices architecture pattern
is determining the correct level of granularity for the service components.

If you find you need to orchestrate your components from user interface - your 
architecture is too fine-graned.
If you need to access multiple components within a single component - your components
are too large.

One way to prevent calling multiple components from within a component is 
to create a `shared database`. This way your services might have repeating 
code for database access, but it's a common practice,
you exchange redundancy of code for decoupling.

## Properties
1. Agility - high
    App built with this pattern tend to be very loosely coupled.
2. Ease of deployment - high
    Easy to deploy due to decoupled nature of pattern
3. Testability - high
    Due to separation of business logic into small components, they are easy to test.
4. Performance - low
    Due to distributed nature of the pattern it's usually not very performant.
5. Scalability - high
    Each component can be individually scaled.
6. Ease of development - high
    High decoupling -> easy development

# Space-based architecture (aka cloud architecture)
Usually architectures are hard to scale.
The space-based architecture pattern is specifically designed to
address and solve scalability and concurrency issues.

The space-based pattern (cloud architecture pattern)
minimizes the factors that limit application scaling

High scalability is achieved by removing the central database constraint and using
replicated in-memory data grids instead.
Because there is no central database, the database bottleneck is
removed, providing near-infinite scalability within the application.

The space-based architecture pattern is a complex and expensive
pattern to implement. It is a good architecture choice for smaller
web-based applications with variable load

However, it is not well suited for traditional
large-scale relational database applications with large amounts
of operational data.

# ■■■ Day 7
# Righting software >>>>>
# Intro
In general design is not time-consuming. If you allocate too much time
for design you risk to add to design things that add nothing but complexity to the design.
Limiting the design time forces you to produce `good-enough` design.

## Eliminating analysis paralysis
1. Design decision `trees`
    Each leaf should be a consistent, distinct and valid solution for a requirement.
    
    When you try to make a new desision about architecture from scratch you
    do something like `bubble sort` which is highly inefficient because it
    doesn't account for previous desisions.
2. Software system design decision tree
    Only after you have designed the system there's a point in designing a project
    to build that system.

    One of the most valuable techniques to decrease the size of a decision tree
    is the application of `constraints`. If you don't have constraints you have
    too many options which is bad (you will spend a lot of time too choose)
    With enough amount of constraints you don't need to design anything, it is what it 
    is.
    
> The clean canvas is the worst design problem. (no constraints)

# System Design <<<
# ■ Desomposition
While designing the system is quick and inexpensive compared with building the system,
it is critical to get the architecture right.

The correct decomposition is critical. A wrong decomposition means wrong architecture.
In modern systems `services` are the most granular unit of the architecture.
However, the technology used to implement the components and their details
are detailed design aspects, not system architecture. 

## Avoid functional decomposition
Functional decomposition decomposes a system into its building
blocks based on the functionality of the system. (Like creating a distinct service
for every operation you need to perform e.g : billing, shipping, checkout)
`Why avoid?`
1. It couples services to the requirements, any change in functionality imposes a change 
    on a service. Functional decomposition precludes reuse
    (you won't be able to use this service in another system)
    and leads to overly compolex systems
    (if you choose to create a single service match-all)

    Functional decomposition, therefore, tends to make services either too big and too few
    or too small and too many. You often see both afflictions in the same system.

2. Clients bloat and coupling
    Functional decomposition often leads to flattening of the system hierarchy.
    Since each service serves specific functionality, someone has to combine these
    discrete functionalities into a required behaviour. That someone is often the client.

    By bloating the client with the orchestration logic,
    you pollute the client code with the business logic of sequencing, ordering,
    error compensation, and duration of the calls to the services. Which prevents
    you from ability to `evolve` client and services `independently`.
    Ultimately, the client is no longer the client—it has become the system.
    
    If there are multiple clients you are destined to duplicate that orchestration 
    logic on all clients making maintenance of all those clients wasteful and expensive.
    As the functionality changes, you now are forced to keep up with that change across
    multiple clients, since all of them will be affected.

3. Multiple points of entry
    Another problem of functional decomposition is that it requires 
    multiple points of entry to the system. The clients need to enter the system 
    in three places: once for the A, then for the B, then for the C service.
    When you will need to change any of these aspects you will need to change it 
    in multiple places across services and clients.

4. Services bloating and coupling
    You can think of letting services call one another instead of multiple points
    of entry for a client. This way you leave only one way of entry for a client 
    which might seem much better.
    The `problem` now is that the functional services are coupled to each
    other and to the `order` in which every service calls each other.

    When you let services know about one another and about order
    in which they should be invoked you couple them so tightly that 
    they become on big fused mess of a service.
    
## Reflecting on functional decomposition
Functional decomposition seems like the perfect way to design a system.
No wonder why so many systems are designed this way.
At all costs, you must `resist the temptations` of functional decomposition.

> Nature of the U niverse
    Functional desomposition is ineffective because it's so simple: just 
    divide the system into requirements and you are done. You can't make a good 
    architecture without breaking a sweat.

Functional decomposition has it's place when trying to figure out requirements 
from the customer. However there should `never` be direct mapping between the
requirements and the design.
    
## Avoid domain decomposition
Domain decomposition is decomposing a system into building blocks
based on the business domains. The reason domain decomposition does not work is that
it is still functional decomposition in disguise.

1. Building a Domain house
    Imagine completely finishing building a single room, you have one room
    with electricity, water and apartment repair. Then you go to your customer
    and show him "release 1.0". Then to build another room you need to `completely`
    rebuild the first one. 
    
    That's how domain decomposition works in pracitce. You can't just build single 
    domain components by themselves without proper architecture. Because if you 
    do, every time you introduce a new component you will likely need to rewrite 
    all other components which increases complexity to `!n`.

## Faulty motivation
The motivation for functional or domain decomposition is that
the business or the customer wants its feature as soon as possible.
The problem is that `you can never deploy a single feature in isolation`.

## Testability and design
A crucial flaw of both functional and domain decomposition has to do with testing. 
With such designs, the level of coupling and complexity is so high
that the only kind of testing developers can do is unit testing. 
The sad reality is that unit testing is borderline `useless`. While unit testing is
an essential part of testing, it cannot really test a `system`.

even if the complex system is at a perfect state of impeccable quality,
changing a single, unit-tested component could break
some other unit relying on old behavior.

The only way to verify change is `full regression testing` of the system
But functional decomposition makes the entire system untestable in terms of 
regression testing (because with so many units it's hard to create a complete list 
of regression tests), and untestable systems are always rife with defects.

## Physical versus software systems
The main difference between physical and software systems is that when person
builds a physical thing it't obvious to everyone whether the thing is well architected
or not. But with software it's not so obvious. Bad architechture is being maintained
by less experienced engineers, slowing down their growth and in some cases ruining their
careers. 

## Example: Functional Trading System
If you design components so that client has to orchestrate them, every change in
any component will likely affect the client as well. Whatever change you may want to 
make will cause substantional rewrite of the existing system.

## Volatility-based decomposition
`Decompose based on volatility`.

Volatility-based decomposition identifies `areas of potential change` and
encapsulates those into services or system building blocks. You then implement
the required behavior as the interaction between the encapsulated areas of volatility

With volatility-based decomposition you can think of your system as of series of vaults
that contain granades (changes) than may potentially cause problems to your app.

With functional decomposition, your building blocks represent
areas of functionality, not volatility so when change happens it may affect multiple
components in your architecture. Functional decomposition doesn't account for `changes`.

## Decomposition, maintenance and development
As explained previously, functional decomposition drastically increases the
system’s complexity. Functional decomposition also makes maintenance a `nightmare`.
It makes maintaining the code labor intensive, error prone, and very time-consuming. 
Even during development it may easily break your deadline because changes are often made
even while development.

## Universal principle
The merits of volatility-based decomposition are not specific to software systems. 
A functional decomposition of your own body would have components for every
task you are required to do, from driving to programming to presenting,
yet your body does not have any such components. 

> Decomposing based on `volatility` is the essence of system design.

All well-designed systems, software and physical systems alike,
encapsulate their volatility inside the system’s building blocks.

# ■■■ Day 8
## Volatility-Based Decomposition and Testing
Volatility-based decomposition lends well to `regression` testing as well as unit testing.
The reduction in the number of components, the reduction in the size of components,
and the simplification of the interactions between components all drastically
reduce the complexity of the system. 

## The Volatility Challenge
The main challenges in performing a volatility-based decomposition have to do with
+ time
+ perception
+ communication

The outside world (be it customers, management, or marketing) always presents
you with requirements in terms of functionality: “The system should do this and that.”
Consequently, `volatility-based decomposition takes longer`
compared with functional decomposition

The whole purpose of requirements analysis is to `identify the areas of volatility`
and this analysis requires effort and sweat.

1. The 2% problem
    If you sick 1 week a year, that's about 2% of your time.
    Would you go to doctor or try to heal yourself? The moral is if you only have 
    to spend 2% of your time at any complex task you will never be good at it.
    Architects should find a way to get more time to architect systems.

2. The Dunning-Kruger effect
    If you are unskilled in something, you never assume it is
    more complex than it is, you assume it is less!

## Identifying volatility
1. Volatile vs variable
    Variability can be handled easily using conditional logic.
    Volatility has a potential to change your whole system logic.

2. Axes of volatility
    Finding areas of volatility is a process of discovery that takes place
    during requirements analysis and interviews with the project stakeholders.

    In any business, there are only two ways (axes) your system could face change: 
        + at the same customer over time. 
        + at the same time over customers
    If something doesn't map to this axes you should not encapsulate it at all
    as it goes back to functional decomposition.

3. Design factoring
    Often, the act of looking for areas of volatility using the axes of
    volatility is an `iterative process`.

4. Independence of the axes
    Two axes should be independent.
    
## Example: Volatility-based decomposition of a house
You may chose these areas of volatility for the first axes (same customer over time): 
    appliances, occupants, furniture, appearance.
And these for the second axes (same time over customers):
    structure, neighbours, location
    
The assignment of a volatility to one of the axes is not an absolute
exclusion but more one of disproportional `probability`.

The axes of volatility are a great starting point,
but it is not the only tool to bring to bear on the problem.

## Solutions masquerading as requirements
E.g when customer says that he wants a kitchen, what he really does is providing
you with a solution to feeding people in the house.

With volatility-based decomposition, during requirements analysis,
you should identify the volatility in feeding the occupants and provide for it
(what's volatile here is the way you provide the food to occupants)

The volatility of feeding is encapsulated within the `Feeding`,
component and as the feeding options change, your design does not.

Since most requirements specifications are chock-full of solutions masquerading
as requirements, functional decomposition absolutely maximizes your pain.
You will forever be chasing the ever-evolving solutions, never recognizing
the true underlying requirements.

## Volatility examples
+ user volatility
+ Client app volatility
    web/mobile
+ security volatility
    different methods of authentication
+ notification volatility
    email/phone/browser
+ storage volatility
    local db / cloud 
    (local db is actually a disguised solution to a requirement of storing data)
+ connection and synchronization volatility
+ locale and regulations volatility
+ input volatility

Is vital to call out the areas of volatility and map them
in your decomposition `as early as possible`

1. System decomposition
    Once you have settled on the areas of `volatility`,
    you need to `encapsulate` them in components of the architecture
    The transition from the list of volatile areas to components of
    the architecture is hardly ever one to one.
    
    With design, always start with the `simple and easy decisions`.
    Those decisions `add constrains` to the system, making subsequent decisions easier.

## Volatility And The Business
While you must encapsulate the volatile areas, not everything that could change
should be encapsulated. During system decomposition, you must identify both
the areas of volatility to encapsulate and those not to encapsulate.
The first indicator is that the possible change is `very rare`.
The second indicator is that any attempt to encapsulate the change can only be done poorly

A change to the `nature` of the business permits you to kill
the old system and start from scratch. You can do it because it happens very rare and
in case it doesn't happen you will save a lot of money that you would otherwise
spend on expensive encapsulating details.

1. Speculative design
    Is a variation on trying to encapsulate the `nature` of the business
    When taken to the extreme, you run the risk of trying to encapsulate anything and
    everywhere, you get some kind of paranoia : "everything can change".
    Your design will have numerous building blocks, a `clear sign of a bad design`.

## Volatility and longevity
The longer the company or the application has been doing something the same way,
the higher the likelihood the company will keep doing it the same way.

## The importance of practicing
The best way of going about mastering volatility-based decomposition is to `practice`.
    + practice on a system that you are familiar with
    + examine your own `past` projects
    + look at your `current` project, it may not be too late to save it
    + look at physical things made by engineers, how do they incapsulate volatility
        (e.g : bicycle, car, computers, TV, smartphones)

Then `do it again and do it some more`.
You better fail in your own projects than on live projects.

# ■ Structure
Fortunately, all systems share common areas of volatility.

## Use cases and requirements
Before diving into architecture, consider requirements.
Requirements should capture the `required behavior` rather than the required functionality
You should specify how the system is required to operate as opposed to what it should do

1. Required behaviours
    A use case is an expression of required behavior—that is,
    how the system is required to go about accomplishing some work
    and adding value to the business. As such, a use case is a particular
    `sequence of activities` in the system.

    You can capture use cases either textually or graphically.
    But because human brain is better prepared for visual information
    the best way of capturing a use case is `graphically`, with a diagram.
    Humans perform image processing astonishingly quickly

    Graphical use cases, however, can be very labor-intensive to produce,
    especially in large numbers. For the most part you shold represent graphically
    usecases that contain "if" in them. Because nested if's are hard to imagine
    without a graphical representation.
    
    1. Activity diagrams
        You should prefer activity diagrams for graphical representation of use cases.
        primarily because they can capture `time-critical` aspects of behavior 
        which allows you to easily represent asynchronous behaviour.

## Layered approach
Software systems are typically designed in layers.
Each layer encapsulates its own volatilities from the layers above
and the volatilities in the layers below. Services inside the layers encapsulate
volatility from each other. 

1. Using services
    The preferred way of crossing layers is by calling `services`. 
    When you use services you gain the following benefits:
        1. Scalability
        2. Security
            All service-oriented platforms treat security as a first-class aspect.
        3. Throughput and availability
            Services can accept calls over queues, allowing you to handle a very
            large volume of messages by simply `queuing` up the excess load.
        4. Responsiveness
            Clients and services can use some reliable messaging protocol
            to guarantee delivery, handle network connectivity issues,
            and even order the calls.
        5. Consistency
        6. Synchronization

## Typical layers
You should consider these four main layers:
    1. Client
    2. Business logic
    3. Resource access
    4. Resource
    (5. Utility `bar`, with helper mehtods like logging, security, message bus and so on)

1. Client layer 
    All clients use the same entry point to the system.
    The client layer encapsulates the potential volatility in Clients
    and allows to develop each client separately. (each client doesn't affect any other)

2. Business logic layer
    The business logic layer encapsulates the volatility in the system’s business logic
    which is the volatility of use cases. A particular use case can change in only
    two ways: 
        + The sequence itself changes 
        + The activities within the use case change
        
    `Both` the sequence and the activities are volatile, and these volatilities 
    should be encapsulated in `specific components`: 
        + **Managers** 
            Should encapsulate vol. in the sequence
        + **Engines**.
            Encapsulate volatility in activity.
    Engines may be shared between Managers because you could perform an activity
    in one use case on behalf of one Manager and then perform the same activity
    for another Manager in a separate use case.
    You should design Engines with `reuse` in mind.
    
    However, if two Managers use two different Engines to perform the same activity,
    you likely have functional decomposition on your hands

3. Resource access layer
    encapsulates the volatility in accessing a resource. Over time, you may want
    to change the way you access the database, so that change or the volatility
    involved should be encapsulated. You also have to not only encapsulate the way
    you access a resource, you also have to encapsulate the vol. in resource itself
    which can be a local, cloud, sql, nosql database.
    
    you must avoid operations such as Open(), Close(), Seek(), Read(), and Write() 
    because these operations are specific to certain kinds of resource access.
    A well-designed ResourceAccess component exposes in its contract
    the `atomic business verbs` around a resource.

    1. Use atomic business verbs
        Atomic business verbs are low level `activities` that can not be expressed 
        by any other activity in the system. E.g in bank may have transfer high level
        activity that comprises of two low level activities: debit and credit. These
        are two atomic business verbs. Altho they are atomic for business they may need
        multiple steps to perform in your system.
        
        Resource access layer should `only` expose these atomic business verbs to the 
        higher layer.

    2. Resource access reuse
        Resource access services can and should be shared between managers and engines
        if you have trouble sharing the same resource access endpoint for the same
        purpose between managers and engines you have probably failed to identify
        an atomic business verb.

4. Resource layer
    contains the actual physical `Resources` on which the system relies
    (such as database or message queue)
    Often, the Resource is a whole system in its own right,
    but to your system it appears as just a Resource.

5. Utilities bar
    contains Utility services, which are some form of common infrastructure
    that nearly all systems require to operate.
    
    Utilities require `different rules` compared with the other components.

## Classification guidelines
1. What's in a name
    Service `names` as well as diagrams are important in communicating your design
    to others. You should follow the following conventions for naming services:
        + names of services should be two-part compound words in PascalCase.
        + suffix of the service is always the service type (manager/engine/access)
        + prefix 
            + for managers - a noun associated with encapsulated volatility
            + for engines - a noun describing encapsulated activity
            + for resource access - noun associated with a resource
        + gerunds should only be used in engines (because they describe activity)
        + atomic business verbs should not be used in prefix for a service name.
            they should only appeare in opeartion names in resource access layer.

    e.g : 
        + BillingManager 
            bad, managers should not contain gerunds, they don't describe actions
            they should describe orchestration of actions.
        + CalculatingEngine
            good
        + AccountEngine
            bad, not clear what activity it incapsulates.

2. The four questions
    When you are not sure about where to start the design, you can start by answering
    four questions about the system:
        1. who
        2. what
        3. how
        4. where

3. Managers to engines ration
    If your design contains a large number of Engines, you may have inadvertently
    done a functional decomposition. Because there is naturally not that many 
    atomic operations in any business.
     
    Typically the should be more managers than engines and they typically
    maintain the `golden ratio` (managers = engines * 1.61). But if you have
    too many manabers (6+) it may be a sign of a bad design. The large number of Managers
    strongly indicates you have done a functional or domain decomposition.

4. Key observations
    1. Volatility decreses top-down
        In a well-designed system, volatility should decrease top-down across the layers.
        Clients are very volatile. Managers do change, but not as much as their Clients.
        Managers change when the use cases change.
        Engines are less volatile than Managers.For an Engine to change, your business
        must change the way it is performing some activity.
        Resources are the least volatile components.
        
        A design in which the volatility decreases down the layers is extremely `valuable`
    
    2. Reuse `increases` top-dows
        Managers are reusable because you can use the same Manager and use cases
        from multiple Clients. Engines are even more reusable than Managers because
        the same Engine could be called by multiple Managers.
    
    3. Proper managers
        If the Manager merely `orchestrates` the Engines and the ResourceAccess,
        encapsulating the sequence volatility, you have a great Manager service.
        
## Subsytems and services
A cohesive interaction between the Manager, Engines, and ResourceAccess
may constitute a single logical service to external consumers. 
You can view such a set of interacting services as a logical `subsystem`. 
(which will still consist of 4 layers, divided `vertically`,
where each vertical slice implements a corresponding set of use cases.)

`Avoid over-partitioning` your system into subsystems.
Most systems should have only a handful of subsystems.
You should limit the number of Managers per subsystem to `three`. 

1. Incremental construction
    In case of building a large-scale application it makes sense to develop
    and deliver the system in stages, one `vertical` slice at a time,
    as opposed to providing a single release at the end of the project.

    With both small and large systems, the right approach to construction
    is another `universal principle`:

    **Design iteratively, build incrementally**

    Meaning design can change because you don't yet know the final result.
    But when you build you need to know what will be the end result of development.
    There are `two reasons` why you can build only incrementally, and not iteratively:
        1. Building iteratively is horrendously wasteful, you can just use a rubber
            gum to remove unwated pieces as you could do at the design stage.
        2. Intermediate iterations don't have any business value.
    
    Building iteratively vs building incrementally
        + iteratively - build 4 floor house but with poor quality, bad walls, bad floors 
            and so on. Then with time imporve all aspects at the same time but slowly.
        + incrementally - build 1 floor properly and then go to the second one 
            and do it properly.
        
    - Extensibility
        The vertical slices of the system also enable you to accommodate extensibility. 
        If you have designed correctly for extensibility, you can mostly
        leave existing things alone and extend the system as a whole by adding more 
        of "vertical slices".
        
2. About microservices
    There are no microservices—only services. 
    1. History and concerns
        In common usage, microservices correspond to domains or subsystems—that is,
        to the vertical slices.
        There are three problems with this idea as practiced today:
            1. Implied constraint on the number of services 
            2. The second problem is the widespread use of functional
                decomposition in microservice design by the industry at large.
                Maintainable, reusable, extensible services are possible
                just not in this way.
            3. The third problem relates to communication protocols.
                Most systems use REST. However REST was designed for publicly
                facing services, as the gateway to systems.  As a general principle,
                in any well-designed system you should never use the same communication
                mechanism both internally and externally.
            
                The protocol used for external services is typically low bandwidth,
                slow, expensive, and error prone. HTTP may be perfect for external
                services, but this protocol should be avoided between internal
                services where the communication and the services must be impeccable.
                
                Internal services such as Engines and ResourceAccess should rely on fast,
                reliable, high-performance communication channels (like message queue)

## Open and closed architecutres
1. Open architecture
    In an open architecture, any component can call any other component
    regardless of the layer in which the components reside. It gives you greater 
    flexibility at a cost of high coupling.
    
    Calling sideways also creates additional coupling. When using open architecture,
    there is hardly any benefit of having architectural layers in the first place.
    In software engineering, trading encapsulation for flexibility is a bad trade.

2. Closed architecture
    In a closed architecture, you strive to maximize the benefits of the
    layers by disallowing calling up between layers and sideways within layers.

3. Semi-closed
    Allows to call multiple levels down. 
    This architecture is justified in two cases: 1. You need maximum performance at 
    all costs. 2. Layers hardly ever change.
    
    You should avoid this architecture as in most cases it can't be justified.

4. Relaxing the rules
    For real-life business systems, the best choice is always a closed architecture. 
    
    1. Calling utilities
        In a closed architecture, Utilities pose a challenge. If you call them a 
        service, not all layers will have an access to it. But specifically for 
        utilities you should relax the closed architecture rule and allow any service
        to call any utility.
        
        You may see attempts by some developers to abuse the utilities bar by
        christening as a Utility any component they wish to short-circuit across
        all layers. Not all components can reside in the utilities bar.
        
        To qualify as a Utility, the component must pass a simple litmus `test`:
        Can it be used in any other complex system?

    2. Calling Resource access by business logic
        Managers and Engines can both call ResourceAccess services without
        violating the closed architecture
    3. Queued manager to manager
        Manager can queue a call to another manager, (it's not a sideways call)
    4. Opening the architecture
        Even with the best set of guidelines, time and again you
        will find yourself trying to open the architecture.
        
        Instead of violating the architecture you should find a better way to 
        do the same thing `preserving the architecture`. There's always such a way.
        E.g you could harness message queuing or utilities bar.

5. Design dont's
    **DON'T DO THESE THINGS**
    1. Clients can only call managers.
        Otherwise you pollute clients with business logic which will render further
        development hell.
    3. Managers don't queue calls to more than one manager in the same use case.
        Use event listening utility instead.
    4. Only managers can queue or listen to events
        Other services should not care about events. It's not their business.
    6. No service should can call it's sibling.

6. Strive for symmetry
    The symmetry in software systems manifests in repeated call patterns across use cases.
    Symmetry is so fundamental for good design that you should generally see the same call
    patterns across Managers.

# ■■■ Day 9
# ■ Composition
How do you know the composition of all the components at run-time
adequately satisfies all the requirements? You can and must be able to produce a
viable design and validate it in a repeatable manner.

## Requirements and changes
Requirements change.

1. Resenting change
    Developers resent changes because when the requirements change,
    their design also must change. In any system, a change to the design is very painful.

2. Design prime directive
    The solution for this dissonance of resenting the changes is so simple
    that it has eluded almost everyone for their entire career:
    **Never design against the requirements**
    Any attempt at designing against the requirements will always guarantee pain.

3. Futility of requirements
    the correct way of capturing the requirements is in the form of `use cases`:
    the required `set of behaviors`(not functions) of the system.
    
    You can't collect a complete and correct list of requirements upfront. requirements
    will change during development.
    
## Composable design
The goal of any system design is to be able to satisfy `all use cases`.
The word all in the previous sentence really means all:
    + present
    + future
    + known
    + unknown 
Nothing less will do. If you fail to pass that bar, then at some
point in the future, when the requirements change, your design will have to change which
is a `sign of a bad design` (architecture should remain constant)

1. Core use cases
    The core use cases represent the essence of the business of the system.
    Most systems have as few as two or three core use cases,
    and the number seldom exceeds six.
    1. Finding the core use cases
        A  core use case will almost always be some kind of an abstraction of other
        use cases, and it may even require a new term or name to differentiate it from
        the rest. The whole point of requirements analysis is
        to `recognize the core use cases` in customer requirements.
    
2. The Architect's mission
    Your mission as an architect is to identify the smallest set of components that
    you can put together to satisfy all the core use cases.
    A composable design does not aim to satisfy any use case in particular.
    1. Architecture validation
        Once you can produce an interaction between your services for each
        `core` use case, you have produced a `valid` design. 

    2. Smallest set
        Remember: Your mission as an architect is to identify not just a set
        of components that you can put together to satisfy all the core use case,
        but the `smallest` set of components.

        The smallest set of services required in a typical software system
        contains number of services that is in order of magnitude `10` (12, 20, 15, 25)
        
        Even in a large system you are commonly looking at two to five Managers,
        two to three Engines, three to eight ResourceAccess and Resources,
        and a half-dozen Utilities.
        
    3. Duration of design effort
        You may spend weeks or months trying to identify the core use
        cases and the areas of volatility. However, that is not design—that is
        requirements gathering and requirements analysis, which may be very
        time-consuming indeed. Design is not time-consuming if you know what you are doing
        You can design in a matter of days, and with practice even faster.

## There's no feature
> Features are always and everywhere `aspects of integration`, not implementation.

Integrating all of automobile parts yields the feature - transporting you from point A to 
point B. You can process text on your laptop but is there any box in
the architecture of the laptop called Word Processing?

The laptop provides the feature of word processing by integrating the keyboard,
the screen, the hard drive, the bus, the CPU, and the memory.

You never should implement feature, you have to impolement everything that combined
will provide the feature and much more. This will make your design solid.

## Handling change
Your software system must respond to changes in the requirements and respond to them fast.

The trick to addressing change is not to fight it, postpone it, or put it altogether.
The trick is `containerizing its effects`  (volatility based decomosition to the resque)

# ■ System design example
## Business alignment
architecture does not exist for its own sake.
The architecture (and the system) must serve the business.
You must ensure that the `architecture is aligned with the vision`
that the business has for its future and with the business objectives

1. The vision
    The vision must drive everything, from architecture to commitments. 
2. Business objectives
    After agreeing on the vision (and only then), you can itemize the
    vision to specific objectives. You should reject all objectives that do
    not serve the vision. 
    E.g objective for the TradeMe system:
        + Quick turnaround for new requirements 
        + high degree of customization between countries
        + support full business visibility 
        + streamline security
    
3. `Mission` statement
    The mission is not to build features—`the mission is to build components`
    that will allow for creating `any feature` now and in the future.
    
    **Vision => Objectives => Mission statement => Architecture**

# ■■■ Day 10
# The Design of Web Apis >>>>>
# Designing predictable API
An API can be predictable because it shares similarities that other
users have encountered before. 

## Being consistent
An inconsistent design introduces variations or contradictions that make
an interface harder to understand and use.

Attaining predictability of a design can be done with
a little bit of discipline by ensuring
`consistency of data and goals` inside and across all APIs, using and meeting prescribed
standards, and by shamelessly copying others.

But if consistency can lead to an awesome API design,
it must `never` be used at the expense of `usability`.

1. Designing consistent data.
    designing consistent APIs starts with choosing consistent `names`
    - inconsistent
        + accountNumber
        + source
        
        + balanceDate
        + dateOfCreation
        + executionDay
    - consistent
        + accountNumber
        + sourceAccountNumber
        
        + balanceDate
        + creationDate
        + executionDate
    When choosing names for various representations of the `same concept`,
    take care to use similar ones.
    Using a `generic suffix` (like balance"Date") or prefix in a name to provide
    additional information about the nature of what is named is a `good` practice.
    
    But even correctly named, a property can still be subject to inconsistency
    if using `inconsistent type` format.
    (e.g different date formats for variables with Date suffix above)
    Choose a `naming convention` and follow it strictly.
    
    Each level of a URL should always have the same meaning
    Create `consistent URLs`:
        - inconsistent
            + accounts/:id
            + transfers/delayed/:id     (delayed part is inconsistent)
        - consistent
            + accounts/:id
            + delayed-transfers/:id

    Create consistent `return values`:
        - inconsistent
            get accounts/
                { "items": [{"accountNumber": "0001234567"}] }
            get transfers/
                [{"id": "00135135"}]            <= different `nesting` and identificator
        
        - consistent
            get accounts/ - [{"id": "0001234567"}]
            get transfers/ - [{"id": "0005232235"}]
            
        
    Basically, `every bit` of an API’s data `must be consistent`.

2. Designing consistent goals
    What’s the problem with the `readAccount` and `getUserInformation` goals? These two
    goal names are inconsistent. Data must be consistent, and so must a goal’s inputs.
    The same goes for the feedback returned in case of success or error return values.
    When designing APIs, you must also take care to create `consistent goal flows`.
    So every single aspect of the interface contract, every behavior of an API,
    must be consistent.
    
3. The four levels of consistentcy
    1. Consisntecy within API
    2. Consistency across organization APIs
    3. Consistency with the domain of an API
        For marine navigation domain you would not use kilometers but 
        nautical miles instead. For language processing you would use `token` instead
        of word.
    4. Consistency with the rest of the world
        (see 4 - copying others)

4. Copying others: Following common practices and meeting standards
    Like any real-world device, an API can take advantage of standards
    (in a broad sense) to be easier to understand
    (almost all real world devices share simillar play button design)
    
    REST APIs can be consistent by simply applying the HTTP protocol’s rules to the
    letter (by using `standartized response codes`)
    
    HTTP protocol provide a consistent framework for REST APIs,
    making them totally predictable (standartized codes and verbs)
    
    Even if not everything is standardized in the API design world 
    (how to create url's properly),
    there are common practices that are very close to being standards.

5. Being consistent is hard and must be done wisely
    You must formally define your design with rules in a document called the
    “API Design Guidelines”. Even if you are the only API designer in the team
    it's very helpful becaus `human tend to forget things`
    
    Once you have your API design cheat sheet and your API directory
    you can concentrate on solving real problems and not waste your time
    reinventing the wheel you created a few months ago.
    
    Consistency is great but don't sacrifice usability for consistency, 
    you can't always be consistent.

## Being adaptable
Managing different representations of the same concept and providing a partial,
selected, or adapted representations of some content is not reserved for books;
we can do that with APIs too.

1. Providing and accepting different formats
    For some array you could provice json view as well as CSV.
    But if a goal can return a list in various formats, how can
    consumers tell which format is needed? You could take advantage of 
    `content negotiation` using `Accept: FORMAT` header.

    Content negotiation is an HTTP mechanism that allows the exchange
    of different representations of a single resource.
    When an HTTP server responds to a request, it must indicate the media type
    of the returned document.
    This is done in the `Content-type` response header.
    When client sends data within request body, it can also use Content-type
    header to indicate the type of data in the body of the request.
    
2. Internationalizing and localizing
    Content negotiation not only applies to data formats, but also to languages.
    Consumers and server can use `Accept-Language` and `Content-Language` headers
    to indicate which language they are speaking and serving in request/response.
    
    `internationalization` means being able to understand
    that an Accept-Language: fr-FR header means that the consumer wants a localized
    response using French language and conventions(e.g measure of distance).
    
    On the server side, it means that if the requested localization is supported,
    the content will be returned localized along with a Content-Language: fr-FR header.
    If it’s not supported, the server returns a 406 Not Acceptable status code.
    `localization` means being actually able to handle some locale.
    
    If you don’t think you need `i18n` and `l10n`, you can start without
    internationalization features and update your API later if needed.

3. Filtering, paginating, and sorting
    To get specific pagin in an array of objects you can utilize `Range` http header.
    (e.g : Range: items=10-19)
    You can use any string value instead of "items" but remember about consistency,
    better choose a name for a single item and stick to it.
    
`Filtering` based on some numeric value
I.e you want to list cars with mileage between 15,000 and 30,000 miles.
You can do it this way: 
    1. add range header specifying 15000 - 30000
    2. add a header specifying what property should be inspected

## Being discoverable
Like a table of contents in books, APIs can be designed in order to be discoverable.
This is done by providing additional data in various ways,
specifically discoverability can be improved by taking advantage of the protocol used.
REST APIs have the discoverable feature in their genes because they use HTTP protocol.

1. Providing metadata
    How do consumers know that there are multiple pages available?
    If you only return items, it's like a book without page numbers and table of content.
    This response could be improved by adding some `metadata` about pagination.
```json
{
    "pagination" : {    // <-- pagination
        "page": 1,
        "totalPages": 9
    },
    "items": {          // <-- items themselves
        
    }
}
```
    Even if client didn't request specific page in a request you could supply 
    this metadata anyway, to make your API discoverable.
    
    Metadata is not limited to pagination, it can be used to tell the client
        + what operations are available on the object
        + what types of objects there are
        + etc.

2. Creating hypermedia APIs
    Imagine a site doesn't have hyperlinks, would it be pleasurable for you
    to got to site docs and discove all hyperlinks yourself?
    The World Wide Web without its hypermedia links would be quite terrible to use.
    
    Fortunately, this isn’t how it works. Once on a website,you can discover its content
    simply by clicking links and going from one page to another. You can leverage
    hyperlinks in your api as well: 
    ```json
    {
        "pagination" : {
            "next" : "/accounts/1234567/transactions?page=2",
            "last" : "/accounts/1234567/transactions?page=7"
        }, 
        "items" : {
            
        }
    }
    ```
    There is no standard way to provide this hypermedia metadata,
    but there are common practices.
    
    Hypermedia metadata usually uses names such as 
        + href
        + links
        + _links.

    Although there is no standard, several hypermedia formats have been defined.
    One of the most popular ones are `HAL` and `Siren`.
    A basic HAL document has a `links` property containing the available links
    Each `link` is an object identified by its relationship with the current resource.
    The link object contains at least an `href` property with the full URL or relative URL
    ```json
    {
        "_links" : {
            "self": {
                "href": "/accounts/1234567"
            },
            "transactions": {
                "href":"/accounts/1234567/transactions"
            }
        },
        "id": "1234567",
        "type": "CURRENT",
        "balance": 10345.4,
        "balanceDate": "2018-07-01"
    }
    ```
    Hypermedia APIs do not only provide available URLs they can also provide
    available HTTP `methods`.
    "Siren" allows you to specify http methods for actions, links for releveant resources
    and properties for the item all in different json objects in a single root json.
    ```json
    // example of a siren document
    {
        "properties" : {
            "id": "000001",
            "date": "2135-07-01",
            "source": "1234567",
            "destination": "7654321",
            "amount": "1045.2"
        },
        "links": [
        { 
            "rel": ["self"],
            "href": "/transfers/000001" 
        }],
        "actions": [
        { 
            "name": "cancel",
            "href": "/transfers/000001",
            "method": "DELETE" 
        }],
    }
    ```
    
    Providing hypermedia metadata is the most common way of taking advantage of the
    web roots of REST APIs to create predictable APIs, but the HTTP protocol provides
    features that can be used to make REST APIs even `more predictable`.

3. Taking advantage of HTTP protocol
    `OPTIONS` http method can be used to request available http methods on a resource:
    ```c
    OPTIONS /transfers/000001       // http request
    
    200 OK                          // http response
    Allow: GET, DELETE              // Allow is an HTTP header
    Content-type: application/json  // Specify content type
    Link: "/accounts/1234567/transactions?type=text/csv" // available formats
    ```
    
    Note that such use of the HTTP protocol by REST APIs is `not widespread`.
    You can use such features, but they will have to be carefully `documented`.
    Don't confuse users with complex or totally unused features.
    
# ■■■ Day 11
# Designing concise and well-organized api
Organizing and sizing an API’s data,
feedback, and goals is important in order to provide an API that can be easily understood
and will not overwhelm users.

## Organizing api
Just like an everyday object, an API can be either unusable or perfectly
intuitive, depending on the `organization` of its data, feedback, and goals.

1. Organizing data
    You can `sort` properties of data by their name.
    You can `group` properties by putting them into another object
    ```json
    {
        "age": 3,
        "type": 2,
        "safeToSpend": 600,
        "overdraftProtection": true,
        "limit": 100
    }
    {
        "overdraftProtection": {
            "active": true,
            "limit": 100
        },
        "age": 3,
        "type": 2,
        "safeToSpend": 600,
    }
    ```

2. Organizing feedback
    A well-organized API provides well-organized feedback.
    You should organize your errors when they occur.
    I.e you could assign to every error it's type (business type), source and message.

3. Organizing goals
    OpenAPI Specification can be used to organize an API’s goals.
    You can add `tags`
    ```yaml
    get: 
      tags: 
         - Account
    ```
    The idea is to group together goals that are related from a functional point of view.
    you must focus on the `functionality` of the goals and not their representations
    
    `Sorting by tags`: people will likely want to accesss account functionality 
    before doing any transfers in some banking app, so you can sort goals by tags
    in OpenAPI like this:
    ```yaml
    opanapi: "3.0.0"
    tags:
        - name: Account
          description: "dfdf"
        - name: Transfer
          description: "dfdf"
    ```
    Now all Account goals will go before Transfer.
    
    Now that the groups are sorted, we should also `sort the operations` within the groups
        1. Preserve the order of http verbs: get, post, patch, delete
        2. put same resources together
        ```openapi
        get beneficiaries
        pust beneficiaries
        get beneficiaries/:id
        ```
        3. group resource paths
        ```
        /account/accounts/
        /account/accounts/{id}
        
        /transfer/beneficiaries/
        /transfer/beneficiaries/{id}
        /transfer/transfers/
        ```

## Sizing api
Objects providing too many functions, too many controls, or too much
information are usually not really usable. Sometimes you’ll find that
what you’d considered as a single API can be worth splitting into different ones

1. Choosing data granularity
    Try to restrict the number of properties and the nesting depth of 
    properties you return to the client.

2. Choosing goal granularity
    You need to consider how particular goal should be structured.
    I.e you should not allow a client to manipulate it's transactions 
    through his account endpoint, instead create another endpoint for 
    transactions manipulating.
    `Avoid` creating does-it-all goals
    
3. Choosing API granularity
    Sometimes you may want to split api into multiple services.
    
# Contextual API design <<<
An API must be `secure by design`

# Designing a secure API






































































































